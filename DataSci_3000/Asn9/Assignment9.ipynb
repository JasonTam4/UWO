{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 9 - GPT\n",
    "\n",
    "In this assignment, you will use various transformer models for semantic search and for language generation. We will be using the `transformers` python package from huggingface; **note** that this package will automatically download language models as required the first time the code is run, and they can be quite large. (The entire assignment might download a few GB.) You might want to do this on campus, depending on your internet situation.\n",
    "\n",
    "This assignment is to be done individually. You may discuss the project with your classmates, but the work you turn in should be your own.\n",
    "\n",
    "\n",
    "# Using Generative Language Models\n",
    "\n",
    "## Goal\n",
    "\n",
    "To learn about how generative language models can be used in practice, focusing on GPT-2, which is feasible to run locally without a graphics card.\n",
    "\n",
    "## Setup\n",
    "\n",
    "This part uses the `transformers` package which can be installed with conda or pip.\n",
    "\n",
    "## Questions (100 pts)\n",
    "\n",
    "1. Write a script that generates a \"story\" using a local GPT-2 model. Your story should: 1) be at least 100 words long; 2) not have repeated phrases; and 3) be the same every time your script is run. It might be nonsensical and/or hilarious. Use the skeleton code provided below as a starting point, and <https://huggingface.co/blog/how-to-generate> as a reference document.\n",
    "\n",
    "## Part 2 Deliverables\n",
    "\n",
    "Submit your notebook as an attachment on OWL as well as a PDF version of the notebook.\n",
    "\n",
    "---\n",
    "\n",
    "# Checklist\n",
    "\n",
    "Your owl submission should include the following attachments and no additional files:\n",
    "```\n",
    "Assignment9.ipynb\n",
    "Assignment9.pdf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers\n",
    "%pip install -q ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display, Markdown\n\u001b[1;32m      3\u001b[0m torch_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, GPT2LMHeadModel, set_seed\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# add the EOS token as PAD token to avoid warnings\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from IPython.display import display, Markdown\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, set_seed\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_decoded_tokens(dt):\n",
    "    display(Markdown(dt))\n",
    "\n",
    "print(\"My GPT-2 Story:\")\n",
    "print(\"---------------\")\n",
    "\n",
    "\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "set_seed(42)  \n",
    "\n",
    "# Generate text with constraints\n",
    "input_ids = tokenizer.encode('Once upon a time', return_tensors='pt').to(torch_device)  # Starting prompt\n",
    "output = model.generate(input_ids, \n",
    "                       max_length=200,  # Adjust to control length\n",
    "                       no_repeat_ngram_size=2,  # Avoids repetition of 2-word phrases\n",
    "                       do_sample=True, \n",
    "                       top_k=50, \n",
    "                       top_p=0.95) \n",
    "\n",
    "# Decode and display the generated story\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "show_decoded_tokens(generated_text)\n",
    "## Replace 'None' with your story; this just wraps the text\n",
    "## to make it easier to read\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
