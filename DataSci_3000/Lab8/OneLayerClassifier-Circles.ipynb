{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This defines a function that plots the X for three classes \n",
    "# and updates the plot\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import collections\n",
    "%matplotlib inline\n",
    "\n",
    "def live_plot(X, y, figsize=(7,5), title=''):\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.scatter(X[y==0].T[0], X[y==0].T[1], color='red')\n",
    "    plt.scatter(X[y==1].T[0], X[y==1].T[1], color='blue')\n",
    "    plt.scatter(X[y==2].T[0], X[y==2].T[1], color='green')\n",
    "    plt.title(title)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pytorch - \n",
    "# If you are in an Anaconda environment, you can use: \n",
    "# conda install pytorch torchvision -c pytorch\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs, make_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a circle data set \n",
    "X, y = make_circles(random_state=0, noise=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And plot it. \n",
    "plt.scatter(X[y==0,0], X[y==0,1], color='red')\n",
    "plt.scatter(X[y==1,0], X[y==1,1], color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In pytorch, a custom network is defined as a \n",
    "# class. If you not familiar with object-oriented programming \n",
    "# and classes in Python read for example: \n",
    "# https://www.python-course.eu/python3_inheritance.php\n",
    "# A Pytorch module has at least a constructor (__init__)\n",
    "# and a function that defines how information is passed through the \n",
    "# network in forward direction. \n",
    "class OneLayerModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, num_classes):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Neural Network Architecture\n",
    "        self.dense1 = torch.nn.Linear(in_features=num_features, out_features=num_classes)\n",
    "        self.activation = torch.nn.LogSigmoid()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.dense1(X)\n",
    "        X = self.activation(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can use the class to make an instance of our network \n",
    "# and let it learn the circle problem \n",
    "num_features = X.shape[1]\n",
    "num_classes = 2\n",
    "model = OneLayerModel(num_features, num_classes) # Generate an instance of our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of numpy arrays, Pytorch uses torch.Tensor \n",
    "# Objects. They behave quite similar to numpy arrays, with the \n",
    "# difference that (Y=X) creates a copy. \n",
    "# Importantly, the allow you to automatically get a derivative \n",
    "# after some operations \n",
    "x1 = torch.tensor([0.0,0.0,1.0,2.0,1.0], requires_grad=True)\n",
    "out = x1.pow(2).sum()\n",
    "out.backward()\n",
    "x1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This also works when two elements are involved \n",
    "x1 = torch.tensor([0.0,0.0,1.0,2.0,1.0], requires_grad=True)\n",
    "x2 = torch.tensor([1.0,1.0,0.0,0.0,0.0], requires_grad=True)\n",
    "out = (x1*x2).sum()\n",
    "out.backward()\n",
    "x1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This even works when the \n",
    "# operations are chained. \n",
    "x1 = torch.tensor([0.0,0.0,1.0,2.0,1.0], requires_grad=True)\n",
    "# x2 = torch.tensor([1.0,1.0,0.0,0.0,0.0], requires_grad=True)\n",
    "x2 = x1.pow(2)\n",
    "out = x2.sum()\n",
    "out.backward()\n",
    "x1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are making our input and desired output pytorch tensors without derivative \n",
    "Xt = torch.FloatTensor(X)\n",
    "yt = torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can get a forward pass \n",
    "y_pred = model.forward(Xt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The loss function is also an object with attached derivative \n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "loss = criterion(input=y_pred, target=yt)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization is implemented in a torch.optim object \n",
    "# here we are using stochastic gradient decent (SGD)\n",
    "# model.parameters() is a link to the parameters of the model, \n",
    "# Which allows the optimize to change it. \n",
    "# input argument lr is learning rate...  \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now put these together and optimise \n",
    "max_iter = 50000\n",
    "\n",
    "for i in range(max_iter):\n",
    "    # Intialize the gradient \n",
    "    optimizer.zero_grad()\n",
    "    # Get current \n",
    "    y_pred = model.forward(Xt) # Get a forward pass with gradient \n",
    "    loss = criterion(input=y_pred, target=yt) # Caluculate the loss  \n",
    "    loss.backward() # propagate the derivative backwards \n",
    "    optimizer.step() # Take one updating step\n",
    "    if i % 100 == 0:\n",
    "        with torch.no_grad():\n",
    "            live_plot(y_pred.detach().numpy(), y, title=f\"Loss at epoch {i}: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the accuracy, we can for computational efficiency run it without gradients \n",
    "with torch.no_grad():\n",
    "    y_pred = model.forward(Xt)\n",
    "    y_pred = torch.argmax(y_pred, dim=1).detach().numpy()\n",
    "    print(f\"Classifier Accuracy: {accuracy_score(y_true=y, y_pred=y_pred) * 100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
