{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataScience 2000B / Integrated Science 2000B Final 2023\n",
    "\n",
    "\n",
    "## Your Student ID: ########\n",
    "\n",
    "## General \n",
    "The instruction for the final exam for DS2000B / IS2002B is included in this Jupyter Notebook. \n",
    "\n",
    "- You are allowed to use any document and source and look up documents on the internet. You are not allowed to use chatGPT, copilot, or any other AI engine that provides programming assistance. \n",
    "- You or not allowed to share documents, or communicate in any other way with people during period of final. Given that there are students with extra time, this is until 6pm.  \n",
    "- You are only allowed to use the python packages listed under \"preliminaries\" - the use of other regression (e.g., statsmodels, numpy polyfit) or machine learning toolboxes (e.g., sklearn) is not permitted. \n",
    "- All the code you are using from previous assignments or labs needs to be included in the notebook. \n",
    "- Most questions also require some written answer. The answer to these questions should be given in full English sentences. \n",
    "- All Figures should be appropriately labeled in x and y axis.  \n",
    "- The Final exam needs to be submitted on OWL (Assignment) before 5:30pm. If you have approved accommodation, you need to submit after 3.5 hrs + your extra time after the start of the exam at 2pm.\n",
    "- Any final submitted later than the alloted time will be scored with 0 pts.  \n",
    "- **It is your responsibility that you submit the correct file. Please check that you uploaded the correct file by downloading the submitted version and opening it in jupyter before you leave the exam room.** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem description \n",
    "\n",
    "Companies are increasingly using Machine Learning algorithms and Artificial Intelligence (AI) to select the job applicants that they want to bring in for an interview. Many applications are nowadays not seen by a human at all anymore - the initial screening is done purely by algorithms. \n",
    "\n",
    "These algorithms work in quite similar ways to the classification algorithms (logistic regression) that you've learned in class: They are models with many parameters, which have been optimized on a training set of data (using a loss function and gradient). The best algorithm will then be picked by testing its predictive performance on a left-out test set. \n",
    "\n",
    "The data the algorithm is trained on a large number of applications from previous job-applicants. It tries to predict whether the job candidate was judged to suitable for the job after the interview. These candidates are shortlisted by the algorithm (shortlist, coded 0: not shortlisted, 1: shortlisted).\n",
    "\n",
    "The main problem with these algorithms is that they are often biased, simply because the original training data was biased. That is, if more white males were invited to an interview than females of ethnic minorities, then the algorithm learns that a name such as \"Alexander\" predicts better chances of being suitable for the job than \"Aaliyah\", even if all the other performance indicators are matched. \n",
    "\n",
    "The company that sells the algorithm claims that the algorithm is unbiased. However, you are more critical, and want to test whether this is really true, before recommending to use the algorithm for hiring. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You decide to conduct an experiment to test the influence of the first name on the decision. \n",
    "Because the algorithm detects when it gets the same application twice (and dismisses them), every single application must be slightly different. Since you have taken Data Science 2000 - this is no problem for you! You write a program that generate 1500 applications, each with a random GPA, degree, University, font, formatting.\n",
    "\n",
    "You then *randomly* assign whether the first name is male or female. You also *randomly choose* whether the first and last name sound White Canadian, or to belong to an ethnic minority (black, first nation, asian, persian). The CVs are then submitted to the algorithm that then gives you a decision of \"shortlisted\" and \"not shortlisted\". \n",
    "\n",
    "The dataset `recruitment.csv` contains the data with the data frame: \n",
    "\n",
    "* age: age of the candidate in years\n",
    "* undergrad_degree: DataSci, CompSci, or Stats\n",
    "* gender: Likely gender indicated by first name on application (male / female)\n",
    "* minority: Whether the name sounds like it belongs to a Canadian of Caucasian descent or not (0: caucasian, 1: non-caucasian)\n",
    "* shortlist: Whether the candidate was shortlisted by the algorithm (0: not shortlisted, 1: shortlisted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminaries - you are only allowed to import the following packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.optimize as so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: (37pts)\n",
    "### Questions 1.1: (3pts)\n",
    "Calculate the probability of being shortlisted by the algorithm with a male and female name. Report the probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender\n",
       "female    0.248266\n",
       "male      0.282413\n",
       "Name: shortlist, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the probability of being shortlisted by the algorithm with a male and female name. (3pts)\n",
    "D = pd.read_csv('recruitment.csv')\n",
    "D.groupby('gender').shortlist.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2: (3pts)\n",
    "Write a function that takes a data frame as input and then returns the difference in probability of being shortlisted by the algorithm between males and females. \n",
    "\n",
    "Call the function with the real data and report the difference in percentage of male and female candidates being shortlisted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct function (2pts) - Note that the difference could be reported the other way around\n",
    "def calc_gender_diff(D):\n",
    "    \"\"\"Calculare the gender difference in the probability of being shortlisted\n",
    "\n",
    "    Args:\n",
    "        D (DataFrame): recruitment data\n",
    "    \"\"\"\n",
    "    m = D.shortlist[D.gender=='male'].mean()\n",
    "    f = D.shortlist[D.gender=='female'].mean()\n",
    "    return m-f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gender difference in the probability of being shortlisted is 3.41%\n"
     ]
    }
   ],
   "source": [
    "# Correct report in percentage (1pts)\n",
    "diff = calc_gender_diff(D)\n",
    "print('The gender difference in the probability of being shortlisted is {:.2f}%'.format(diff*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3: (8pts)\n",
    "In this question, you will determine whether the gender difference found in Q.1.2 is significant.\n",
    "\n",
    "You want to test the hypothesis that the algorithm takes the (likely) gender of the first name on the application into account when making shortlisting decisions. \n",
    "\n",
    "* Formulate the Null-hypothesis\n",
    "* Conduct a randomization test where you randomly randomly shuffle the column of the dataframe that is exchangeable under the Null hypothesis. \n",
    "* Plot a histogram of the difference in probability between male and female candidates. \n",
    "* Choose whether you are conducting a one-sided or two-sided test\n",
    "* Report the p-value for testing the the hypothesis and draw a conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Null hypothesis: The algorithm's shortlisting decision is not influence by the gender of the first name on the CV (2pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_column(df,colname):\n",
    "    \"\"\"Random\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    var = df_copy[colname].values\n",
    "    np.random.shuffle(var)\n",
    "    df_copy[colname]=var\n",
    "    return df_copy\n",
    "\n",
    "def randomization_test(D,fcn,shuffle,numIterations=500,sides=1, nbins = 10):\n",
    "    listOfTS =  np.array(range(numIterations),dtype = 'float64')\n",
    "    for i in range(numIterations):\n",
    "        #1. Randomly shuffle the data\n",
    "        S= randomize_column(D,shuffle)\n",
    "        #2. Calculate test statistics\n",
    "        listOfTS[i] = fcn(S)\n",
    "\n",
    "    # 3. Calculate the real test statistic\n",
    "    realTS = fcn(D)\n",
    "\n",
    "    # 4. Plot a histogram of the\n",
    "    plt.hist(listOfTS,bins= nbins)\n",
    "    plt.axvline(x=realTS, color='k')\n",
    "\n",
    "    # 5. determine p-value (one sided) with rejection region above the threshold\n",
    "    if sides==1:\n",
    "        p=sum(listOfTS>=realTS)/len(listOfTS)\n",
    "    elif sides==-1:\n",
    "        p=sum(listOfTS<=realTS)/len(listOfTS)\n",
    "    elif sides==2:\n",
    "        plt.axvline(x=-realTS, color='r')\n",
    "        p=sum(np.absolute(listOfTS)>=np.absolute(realTS))/len(listOfTS)\n",
    "    print(' P-value of the randomization test is p= ',p)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " P-value of the randomization test is p=  0.1508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1508"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "randomization_test(D,calc_gender_diff,'gender',numIterations=5000,sides=2, nbins = 40) # Using randomization correctly 2 pts - using 2-sided test: 2 pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Conclusion: (2pts)*\n",
    "Based on the p-value (p=0.1478) we cannot reject the Null-hypothesis that the algorithm is not biased by the name of the candidate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.4 (5pts)\n",
    "In this question we will test whether the algorithm is biased against women of ethnic minorities. We will do this by comparing the probability of being shortlisted for a application with a female minority name to anybody else. \n",
    "\n",
    "Add a new column entitled 'female_minority' that is 1 if the name indicates the the applicant is both female AND a member of a minority, and 0 otherwise. \n",
    "\n",
    "Write a function that returns the difference in the probability of getting shortlisted for female members of an ethnic minority against anybody else. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coded columns correct: 2 pts\n",
    "D['female_minority'] = np.logical_and(D.gender=='female',D.minority==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New function and correct answer: 3 pts\n",
    "def calc_female_minority_diff(D):\n",
    "    \"\"\"Calculate the difference in the probability of being shortlisted when name is female + minority\n",
    "\n",
    "    Args:\n",
    "        D (DataFrame): recruitment data\n",
    "    \"\"\"\n",
    "    y = D.shortlist[D.female_minority==1].mean()\n",
    "    n = D.shortlist[D.female_minority==0].mean()\n",
    "    return y-n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference in the probability of being shortlisted is -8.48%\n"
     ]
    }
   ],
   "source": [
    "diff = calc_female_minority_diff(D)\n",
    "print('The difference in the probability of being shortlisted is {:.2f}%'.format(diff*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.5: (7pts)\n",
    "In this question you will determine whether the difference observed in Q.1.4 constitute significant discrimination against women of ethics minorities. \n",
    "\n",
    "You want to test the hypothesis that the algorithm is *less likely* to shortlist the candidate, if the name on the application suggests that the applicant is both female and a member of a minority.  \n",
    "\n",
    "* Formulate the Null-hypothesis \n",
    "* Conduct a randomization test (with at least 5000 iterations) where you randomly randomly shuffle the column of the dataframe that is exchangeable under the Null hypothesis. \n",
    "* Plot a histogram of the difference in probability between male and female candidates. \n",
    "* Choose whether you are conducting a one-sided or two-sided test\n",
    "* Report the p-value for testing the the hypothesis and draw a conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Null hypothesis: the algorithm is *equally or more likely* to shortlist the application, if the name on the application suggest that the applicant is female and minority. (2pts, 0pts if it does not mention >=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " P-value of the randomization test is p=  0.0006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0006"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "randomization_test(D,calc_female_minority_diff,'female_minority',numIterations=5000,sides=-1, nbins = 40) # 3pts - (1pts for 1-sided + 2pts reading it off the correct side)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The p-value (p=0.0006) allows us to reject the Null-hypothesis.  (*2 pts for correct conclusion. Can get up to 1pts if drawing correct conclusion on wrong p-value*). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: (25pts)\n",
    "\n",
    "Instead of just doing a randomization test to test your hypotheses, you decide that you also need to account for possible other differences between the applications. \n",
    "Even though you randomly assigned the name to each application, you cannot be sure that they are exactly matched between the groups. \n",
    "So you decided to build and test different models of the shortlisting decision of the algorithm. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.1: (4pts) \n",
    "\n",
    "Create dummy coded variables for gender (male / female) \n",
    "and undergraduate major (DataSci, CompSci, Stats).\n",
    "\n",
    "You are free to choose on of the variables as comparison group - just be aware that it will influence the your interpretation of the regression coefficients later on. \n",
    "\n",
    "Based on your dummy variable, report the proportion of applications that have labels with having a \"Data Science\" Major. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dummy code gender (1pts)\n",
    "D['genderI']=np.double(D['gender']=='female')\n",
    "\n",
    "# Dummy code major (2pts)\n",
    "D['DataSciI']=np.double(D['undergrad_degree']=='DataSci')\n",
    "D['CompSciI']=np.double(D['undergrad_degree']=='CompSci')\n",
    "\n",
    "# Report correct proportion (1pts)\n",
    "D.DataSciI.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "34% of the applications are listed to have a Data Science degree. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2: Build regression model (9pts)\n",
    "\n",
    "Build a regression model to predict the shortlisting decision (1/0) of the algorithm based on the following variables:\n",
    "\n",
    "* gender (dummy coded)\n",
    "* undergrad_degree (dummy coded)\n",
    "* minority\n",
    "* GPA \n",
    "\n",
    "\n",
    "Written answer: From among multiple Regression, median regression, and logistic regression, choose the most appropriate regression technique and justify your choice. \n",
    "\n",
    "Estimate and report the parameters of your model. \n",
    "\n",
    "Written answer: Interpret the gender, minority, and undergraduate degree effects. Which of the groups does the algorithm have a preference for? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Picking logistic regression and justification: 2pts*\n",
    "The y-variable is a Bernoulli (binary) variable, therefore the most appropriate regression technique is logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticRegPredict(b,D,xname):\n",
    "    yp=np.ones(len(D.index))*b[0]       # Start out with the intercept\n",
    "    for i in range(len(xname)):\n",
    "        yp=yp+D[xname[i]]*b[i+1]        # Add the prediction of each regressor separately\n",
    "    p = np.exp(yp)/(1+np.exp(yp))\n",
    "    p = p.clip(1e-12,1-(1e-12))\n",
    "    return p\n",
    "\n",
    "def logisticRegLoss(b,D,y,xname):\n",
    "    p = logisticRegPredict(b,D,xname)\n",
    "    cost = -y*np.log(p)-(1-y)*np.log(1-p)\n",
    "    N=len(xname)\n",
    "    grad=np.zeros(N+1)\n",
    "    res = y-p\n",
    "    grad[0]=-sum(res)\n",
    "    for i in range(N):\n",
    "        grad[i+1]=-np.sum(D[xname[i]]*res)         # Add each regressor\n",
    "    return (cost.sum(),grad)\n",
    "\n",
    "def logisticRegFit(D,y,xname,figure=0,b0=[]):\n",
    "    k=len(xname)+1\n",
    "    if (len(b0)!=k):\n",
    "        b0=np.zeros(k)\n",
    "    RES = so.minimize(logisticRegLoss,b0,args=(D,y,xname),jac=True)\n",
    "    b = RES.x\n",
    "    ll = -RES.fun # Negative function value is the log likelihood\n",
    "    p = logisticRegPredict(b,D,xname)\n",
    "    if (k==2 & figure==1):\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "        ax.scatter(D[xname[0]],y)\n",
    "        xRange=[min(D[xname[0]]),max(D[xname[0]])]\n",
    "        xp=np.arange(xRange[0],xRange[1],(xRange[1]-xRange[0])/50)\n",
    "        yp=b[0]+b[1]*xp\n",
    "        pp=np.exp(yp)/(1+np.exp(yp))\n",
    "        ax.plot(xp,pp,'r-')\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "    return (ll,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-857.0487989043158,\n",
       " array([-1.76478355, -0.19834488, -0.33527128,  0.21011037, -0.20019992,\n",
       "         0.31107727]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# *Correct functions and use of logistic regression - report of correct (3pts)*\n",
    "logisticRegFit(D,D.shortlist,['genderI','minority','DataSciI','CompSciI','GPA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the regression slope for the following variables the algorithm has a preference for: \n",
    "\n",
    "* GenderI = -0.19: Preference against females (for males): 1pts\n",
    "* Minority = -0.33: Preference against non-caucasians (for caucasians): 1pts\n",
    "* DataSci = 0.21: Preference for Data Science majors over others: 1pts \n",
    "* CompSci = -0.20: Preference against CompSci majors over others: 1pts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3: (8pts)\n",
    "Use Model comparison to determine whether gender has an effect on the probability of being shortlisted, after we account for the variables (minority, undergrad_degree, and GPA). \n",
    "\n",
    "Compare the models using 50-fold cross-validation to determine the cross-validated prediction performance the two appropriate model.\n",
    "\n",
    "Is there positive evidence (in the sense of Kass & Raftery, 1995) for an effect of gender on the algorithms decision? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correct use of function (2pts)\n",
    "def KfoldCVlogisticReg(D,y,xname,K=50,fitfcn=logisticRegFit,predictfcn=logisticRegPredict):\n",
    "    N = len(y) #Number of observations\n",
    "    yp= np.zeros(N)\n",
    "    ind = np.arange(N)\n",
    "    ind = np.floor(ind/N*K)\n",
    "\n",
    "    # Get overall model fit\n",
    "    LL,b_all=fitfcn(D,y,xname,figure=0)\n",
    "\n",
    "    # Loop over the cross-validation folds\n",
    "    for i in range(K):\n",
    "        r,b=fitfcn(D[ind!=i],y[ind!=i],xname,b0=b_all,figure=0)\n",
    "        yp[ind==i]=predictfcn(b,D[ind==i],xname)\n",
    "    LLcv = sum(y*np.log(yp)+(1-y)*np.log(1-yp))\n",
    "    return LLcv,LL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Bayes factor in favor of the model including gender is:1.3060013957386516\n"
     ]
    }
   ],
   "source": [
    "# Choice of correct terms for both models (2pts)\n",
    "LLcv1,LL1 = KfoldCVlogisticReg(D,D.shortlist,['genderI','minority','DataSciI','CompSciI','GPA'])\n",
    "LLcv0,LL0 = KfoldCVlogisticReg(D,D.shortlist,['minority','DataSciI','CompSciI','GPA'])\n",
    "\n",
    "# Correct Bayes factor (2pts)\n",
    "print(f'The Bayes factor in favor of the model including gender is:{np.exp(LLcv1-LLcv0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Correct conclusion: (2pts)*\n",
    "\n",
    "> While the model including gender has better predictive performance of the algorithms decision, in a bayesian sense, there is no positive evidence for the influence of gender on the decision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.4 (4pts)\n",
    "Now use the same approach as in Question 2.3 to test whether being a female AND a member of minority has an influence of the algorithms decision, after we  account for the influence of GPA and undergraduate degree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Bayes factor in favor of the model including gender is:77.6986652995251\n"
     ]
    }
   ],
   "source": [
    "# Choosing the correct model to compare: 2pts\n",
    "LLcv0,LL0 = KfoldCVlogisticReg(D,D.shortlist,['DataSciI','CompSciI','GPA'])\n",
    "LLcv2,LL2 = KfoldCVlogisticReg(D,D.shortlist,['female_minority','DataSciI','CompSciI','GPA'])\n",
    "print(f'The Bayes factor in favor of the model including gender is:{np.exp(LLcv2-LLcv0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *correct Bayes factor (1pt) and correct conclusion (1pt)*\n",
    "According to Kaas and Raftery (1995) there is strong evidence that the algorithm is less likely to shortlist women of minorities, even if we account for GPA and the undergraduate degree.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Regularized logistic regression (19pts)\n",
    "Because there are so many potential variables that could influence the algorithm's decision, you want to implement a L2-regularized version of logistic regression. \n",
    "Having taken Data Science 2000, you decide to do it the hard (and heroic) way, and write the code yourself. That way you can be sure your really know what is going on!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1  (3pts)\n",
    "\n",
    "Z-standarize the variables GPA, age, gender (dummy coded), minority (dummy coded). \n",
    "\n",
    "Check that the variables have after standardization (approximately) a mean of 0 and a standard deviation of 1 by reporting the mean and standard deviations of one of the variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 1.675696618500903e-16\n",
      "Std: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Z-standardize the data: correct function (1pts)\n",
    "def zstandardize(d):\n",
    "    d = (d-d.mean())/d.std()\n",
    "    return d\n",
    "\n",
    "# Correct use of function (1pts)\n",
    "D['ageZ'] = zstandardize(D.age)\n",
    "D['GPAZ'] = zstandardize(D.GPA)\n",
    "D['genderZ'] = zstandardize(D.genderI)\n",
    "D['minorityZ'] = zstandardize(D.minority)\n",
    "\n",
    "# Correct check 1pt\n",
    "print('Mean:',D.ageZ.mean())\n",
    "print('Std:',D.ageZ.std())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2 (7pts)\n",
    "Change the loss function for logistic regression (i.e. see Assignment 11 solutions) to include L2-regularization.\n",
    "\n",
    "Regularize all regression coefficients except the intercept with a regularization coefficient of alpha = 0.1. \n",
    "Adjust the gradients accordingly.\n",
    "\n",
    "Calculate the loss and gradients for the regularized logistic regression model, using the variables: \n",
    "\n",
    "* Intercept \n",
    "* age (z-standardized)\n",
    "* GPA (z-standardized)\n",
    "* gender (dummy coded and z-standardized)\n",
    "* minority (dummy coded and z-standardized)\n",
    "\n",
    "\n",
    "Report the loss and gradient for the regularized model (alpha = 0.1) and compare it to the loss for the unregularized model for the parameters \n",
    "`b= [-0.5,0.2,0.3,-0.2,-0.2]`\n",
    "(these are the regression coefficients for [intercept, ageZ, GPAZ, genderZ, minorityZ])\n",
    "\n",
    "Written answer: \n",
    "   * Why does the regularized model have a higher loss than the non-regularized model?\n",
    "   * Which direction do you have to change the regression coefficient for the intercept to lower the loss? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct addition to loss (1pts), correct addition to gradient (2pts)\n",
    "def logisticL2Loss(b,D,y,xname,alpha):\n",
    "    p = logisticRegPredict(b,D,xname)\n",
    "    cost = np.sum(-y*np.log(p)-(1-y)*np.log(1-p)) + alpha*np.sum(b[1:]**2)\n",
    "    N=len(xname)\n",
    "    grad=np.zeros(N+1)\n",
    "    res = y-p\n",
    "    grad[0]=-sum(res)\n",
    "    for i in range(N):\n",
    "        grad[i+1]=-np.sum(D[xname[i]]*res) + 2*alpha*b[i+1]       # Add each regressor\n",
    "    return (cost,grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900.4443271446819,\n",
       " array([175.48493717, -31.69427383,  61.37045306, -41.00517858,\n",
       "        -18.84360863]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correct results of regularized function (1pts)\n",
    "b=np.array([-0.5,0.2,0.3,-0.2,-0.2])\n",
    "logisticL2Loss(b,D,D.shortlist,['ageZ','GPAZ','genderZ','minorityZ'],alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900.4233271446819,\n",
       " array([175.48493717, -31.73427383,  61.31045306, -40.96517858,\n",
       "        -18.80360863]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correct comparison (1pts)\n",
    "b=np.array([-0.5,0.2,0.3,-0.2,-0.2])\n",
    "logisticRegLoss(b,D,D.shortlist,['ageZ','GPAZ','genderZ','minorityZ'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The loss for the regularized model is larger, as it is the normal logistic regression loss PLUS the sum of squared regression coefficients. The latter term is always positive.  (1pts)\n",
    "\n",
    "> The derivative of the loss in respect to the intercept is positive - so we need to lower the intercept to minimize the loss. (1pt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.3 (9pts)\n",
    "Change the logisticRegFit from Assignment 10/11 to use regularization. \n",
    "Fit the the model from question Q3.2 (age, GPA, gender, minority), using L2 regularization with alpha = 0.1. \n",
    "\n",
    "The function should return the log-likelihood of the data given the model (*Hint: In this case this is not anymore equal to the negative loss!*) \n",
    "\n",
    "Compare the logliklihood and the parameters to the fit of the unregularized model.\n",
    "\n",
    "Written answer: What can you conclude from the comparison of the log-likelihood? Can you determine which model is better?\n",
    "\n",
    "Written answer: What is the difference between the regression coefficients of the regularized and unregularized model? How can you explain this difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct loss function (1pts) and correct likelihood returned (2pts)\n",
    "def logisticL2Fit(D,y,xname,figure=0,b0=[],alpha=2.0):\n",
    "    k=len(xname)+1\n",
    "    if (len(b0)!=k):\n",
    "        b0=np.zeros(k)\n",
    "    RES = so.minimize(logisticL2Loss,b0,args=(D,y,xname,alpha),jac=True)\n",
    "    b = RES.x\n",
    "    ll,_= logisticRegLoss(b,D,y,xname) # log likelihood\n",
    "    ll= -ll # Negative of negative log likelihood is the log likelihood\n",
    "    return (ll,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood:-844.5831\n",
      "[-1.0549234   0.34403319  0.14524632 -0.10133384 -0.17202798]\n"
     ]
    }
   ],
   "source": [
    "# Correct results for regularized version (1pts)\n",
    "ll,b=logisticL2Fit(D,D.shortlist,['ageZ','GPAZ','genderZ','minorityZ'],alpha=0.1)\n",
    "print(f'Log-likelihood:{ll:.4f}')\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood:-844.5831\n",
      "[-1.05498408  0.34429864  0.14535801 -0.10141789 -0.17216197]\n"
     ]
    }
   ],
   "source": [
    "# Correct results for non-regularized version (1pts)\n",
    "ll,b=logisticRegFit(D,D.shortlist,['ageZ','GPAZ','genderZ','minorityZ'])\n",
    "print(f'Log-likelihood:{ll:.4f}')\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The log-likelihood of the regularized model is lower than the non-regularized model. This is natural, as the regularized model was trained to minimize a loss that included a regularization term, not just the log-likelihood. To compare the models we need to employ cross-validation to get a predictive performance estimate. (2pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The regression coefficients are smaller in the regularized model. This is because the regularized model was trained to minimize the loss function, which includes a regularization term. The regularization term penalizes large regression coefficients. (2pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congrats: This is the end! You are done with the Final and Data Science 2000. This was a challenging course, and if you finished the final decently, you can be proud of yourself!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
