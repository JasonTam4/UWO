{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 11: Model Selection, Regularized regression and final practice\n",
    "## Learning goals \n",
    "This homework includes some repeated task and variations on model fitting, and model comparison, which should prepare you optimally for the final. \n",
    "The homework also introduces z-standardization and regularized (L2) regression. \n",
    "Try to solve the task in the homework independently, and prepare a cheat-sheet and a file with useful functions, such that you can complete this homework in 3 hrs or less. \n",
    "\n",
    "## Data set \n",
    "The file kaiser.csv contains a subset of data from the Child Health and Development Studies, which investigate a range of topics. One study considered all pregnancies between 1960 and 1967 among women in the Kaiser Foundation Health Plan in the San Francisco East Bay area. Here, we look at the predictor of birth weight of babies, measured in pounds, as well as the occurrence of complications in the first 3 month. \n",
    "\n",
    "The data frame stored in kaiser.csv contains the variables: \n",
    "- age:          Age of the mom at time of birth\n",
    "- smoke:        Is the mom a smoker / non-smoker? \n",
    "- hospital:     Which hospital was the birth at? Oakland, SanFrancisco, WalnutCreek, SanJose, and Richmond.\n",
    "- gestation:    Gestation period (length of pregnancy) [days]\n",
    "- parity:       1: child the first born 0: Child has older siblings \n",
    "- weight:       Weight of the baby [pounds].  \n",
    "- complication: Was there a complication within the first 3 month of pregnancy (0: No 1:Yes) \n",
    "\n",
    "## Preliminaries\n",
    "Set up the environment by importing pandas, seaborn, numpy, scipy.optimize and matplotlib. \n",
    "Then add your multiple regression functions (multRegPredict,multRegLossRSS, multRegFit) from the last homeworks. \n",
    "\n",
    "To make it easier - we have done these things already! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as so\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multRegPredict(b,D,xname):\n",
    "    \"\"\"Prediction function for multiple regression\n",
    "\n",
    "    Args:\n",
    "        b (nd.array): Array of regression coefficients - first is intercept\n",
    "        D (pd.DataFrame): Pandas data frame with explanatory variables\n",
    "        xname (list): List of strings with names of explanatory variables\n",
    "\n",
    "    Returns:\n",
    "        yp (nd.array): Predicted y - values\n",
    "    \"\"\"\n",
    "    yp=np.ones(len(D.index))*b[0]        # Intercept\n",
    "    for i in range(len(xname)):\n",
    "        yp=yp+D[xname[i]]*b[i+1]         # Add each regression value\n",
    "    return yp\n",
    "\n",
    "def multRegLossRSS(b,D,y,xname):\n",
    "    \"\"\"Loss function for OLS multiple regression\n",
    "\n",
    "    Args:\n",
    "        b (nd.array): Array of regression coefficients - first is intercept\n",
    "        D (pd.DataFrame): Pandas data frame with explanatory variables\n",
    "        y (ndarray): Dependent variable\n",
    "        xname (list): List of strings with names of explanatory variables\n",
    "\n",
    "    Returns:\n",
    "        rss: Current loss\n",
    "        grad: gradient of loss function in respect to parameters\n",
    "    \"\"\"\n",
    "    predY = multRegPredict(b,D,xname)\n",
    "    res = y-predY\n",
    "    rss = sum(res**2)\n",
    "    grad=np.zeros(len(b))\n",
    "    grad[0]=-2*np.sum(res)\n",
    "    for i in range(len(xname)):\n",
    "        grad[i+1]=-2*np.sum(D[xname[i]]*res)\n",
    "    return (rss,grad)\n",
    "\n",
    "def multRegFit(D,y,xname=[],figure=0,b0=[]):\n",
    "    \"\"\"Fits a multiple regression loss function\n",
    "\n",
    "    Args:\n",
    "        D (pd.DataFrame): Pandas data frame with explanatory variables\n",
    "        y (ndarray): Dependent variable\n",
    "        xname (list): List of strings with names of explanatory variables\n",
    "        figure (int): Plot figure? Defaults to 0.\n",
    "        b0 (np.ndarray). Initial guess for the parameter vector\n",
    "\n",
    "    Returns:\n",
    "        R2: Fitted R2 value\n",
    "        b: Fitted\n",
    "    \"\"\"\n",
    "    k=len(xname)+1\n",
    "    if (len(b0)!=k):\n",
    "        b0=np.zeros((k,))\n",
    "    RES = so.minimize(multRegLossRSS,b0,args=(D,y,xname),jac=True)\n",
    "    b=RES.x # Results\n",
    "    res = y-np.mean(y)\n",
    "    TSS = sum(res**2)\n",
    "    RSS,deriv = multRegLossRSS(b,D,y,xname)\n",
    "    R2 = 1-RSS/TSS\n",
    "    if (k==2 and figure==1):\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "        ax.scatter(D[xname[0]],y)\n",
    "        xRange=[min(D[xname[0]]),max(D[xname[0]])]\n",
    "        xp=np.arange(xRange[0],xRange[1],(xRange[1]-xRange[0])/50)\n",
    "        yp=b[0]+b[1]*xp\n",
    "        ax.plot(xp,yp,'r-')\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "    return (R2,b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Multiple regression with discrete variables ( / 30 pts)\n",
    "### Question 1.1 ( / 10pt)\n",
    "Create a dummy variable for Smoker / Non-smoker. Set the value for “Smoker” to 1 and for “Non-smoker” to 0. Estimate a regression model with the dummy variable as a regressor *and birth weight as the response variable.* Use the RSS as the loss function. \n",
    "\n",
    "Report the value of the intercept and slope. \n",
    "\n",
    "Written answer: What does the intercept and slope value indicate? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = pd.read_csv('kaiser.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "D['smokeDummy'] = np.double(D['smoke'] == 'smoker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The intercept is 7.69\n",
      "The slope is -1.21 \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "r2, b = multRegFit(D,D['weight'],['smokeDummy'],figure=1,b0=[])\n",
    "plt.xlabel('smoke dummy variable')\n",
    "plt.ylabel('birth weight')\n",
    "print(f'The intercept is {b[0]:.2f}')\n",
    "print(f'The slope is {b[1]:.2f} ' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure 1*. Shows the scatter plot of birth weight of babies with smoking mothers (1) and non-smoking moms (0). The red line is the regression line."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> b0 (the intercept) is the mean of the non-smoking group (y = b[0] + b[1] * 0) and b[1] is the difference between the means of smokers and non-smokers (y = b[0] + b[1] * 1). Since b[1] is a negative value, the mean birth weight of babies of smoker group is lower than the mean of the non-smoker group."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2 ( / 8pt)\n",
    "Make a boxplot of hospital on the x-axis and birthweight on the y-axis (see HW2 for an example). Which hospital has the lowest overall birth weight?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='hospital', ylabel='weight'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(x=D.hospital,y=D.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure 2*. Boxplot of birth weight of babies grouped by the hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hospital\n",
       "Oakland         7.441426\n",
       "Richmond        7.313273\n",
       "SanFrancisco    7.667122\n",
       "SanJose         7.544500\n",
       "WalnutCreek     7.645780\n",
       "Name: weight, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calucate the mean weight for each hospital\n",
    "D.groupby('hospital').mean()['weight']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Richmond has the lowest average birth weight. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3  ( / 12pt)\n",
    "Create a set of 4 dummy variables that together code the hospital. Set Walnut Creek to be your comparison group. Run a multiple regression model with the 4 dummy variables as explanatory variables. Report the interecept and slope values. What do the intercept and slope values mean? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "D['hospOAK'] = np.double(D.hospital=='Oakland')\n",
    "D['hospSFO'] = np.double(D.hospital=='SanFrancisco')\n",
    "D['hospSJO'] = np.double(D.hospital=='SanJose')\n",
    "D['hospRIM'] = np.double(D.hospital=='Richmond')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B:7.646\n",
      "B:-0.204\n",
      "B:0.021\n",
      "B:-0.101\n",
      "B:-0.333\n"
     ]
    }
   ],
   "source": [
    "r2,b=multRegFit(D,D.weight,['hospOAK','hospSFO','hospSJO','hospRIM'])\n",
    "print(f\"B:{b[0]:3.3f}\")\n",
    "print(f\"B:{b[1]:3.3f}\")\n",
    "print(f\"B:{b[2]:3.3f}\")\n",
    "print(f\"B:{b[3]:3.3f}\")\n",
    "print(f\"B:{b[4]:3.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The incept is 7.429, which is the average weight for WalnutCreek<br> \n",
    "The slope for Oakland is -0.204, which means that the birth weight of babies in Oakland is 0.2 pounds lower than in WC <br>\n",
    "The slope for SFO is 0.021, which means that the birth weight of babies in SF is 0.021 pounds higher than in WC<br>\n",
    "The slope for SJO is -0.101, which means that the birth weight of babies in San Jose is 0.101 lower than in WC<br>\n",
    "The slope for Richmond is -0.33, which means that the birth weight of babies in Richmond is 0.33 pounds lower than in WC<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Model selection for multiple regression  ( / 35 pts)\n",
    "### Question 2.1 ( / 20 pts)\n",
    "Write a version of the crossvalidation function that does K-fold crossvalidation and works specifically with multRegFit as the fitting function. \n",
    "\n",
    "KfoldCVmultReg(D,y,xname,K=20,fitfcn=multRegFit,param={},predictfcn=multRegPredict):\n",
    "- D: Data Frame with explanatory variables  \n",
    "- y: response variable \n",
    "- xname: List of explanatory variables\n",
    "- K: Number of crossvalidation folds\n",
    "\n",
    "For dividing the data up in K pieces, you can use the following trick to assign a partition index to each of the data-points:\n",
    "```\n",
    "#N = number of data points \n",
    "#K = number of test sets (folds)\n",
    "ind = np.arange(N)\n",
    "ind = np.floor(ind/N*K)\n",
    "```\n",
    "\n",
    "\n",
    "The code should compute and return the predictive (crossvalidated) $R^2$ (R2cv) and the fitted $R^2$ (R2). \n",
    "It should use the entries in the Dictionary to pass them to the function using `fitfcn(D,y,xname,**param)`\n",
    "Run 20-fold crossvalidation on the multiple regression model, with birth weight as the response variable and \n",
    "\n",
    "- age of the mother \n",
    "- smoker (dummy coded) \n",
    "- birth occurred in Oakland? \n",
    "- gestation \n",
    "\n",
    "as explanatory variables. \n",
    "Report R2cv and R2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KfoldCVmultReg(D,y,xname,K=20):\n",
    "    \"\"\"K-fold Crossvalidation for multiple regression\"\"\"\n",
    "    N = len(y) #Number of observations\n",
    "    yp= np.zeros(N)\n",
    "\n",
    "    # Make an index vector with K folds\n",
    "    ind = np.arange(N)\n",
    "    ind = np.floor(ind/N*K)\n",
    "\n",
    "    # Get overall model fit\n",
    "    R2,b_all=multRegFit(D,y,xname)\n",
    "\n",
    "    # Loop over the crossvalidation folds\n",
    "    for i in range(K):\n",
    "        r,b=multRegFit(D[ind!=i],y[ind!=i],xname,b0=b_all)\n",
    "        yp[ind==i]=multRegPredict(b,D[ind==i],xname)\n",
    "\n",
    "    # Calculate crossvalidated model fit\n",
    "    TSS  = sum((y-y.mean())**2)\n",
    "    RSScv = sum((y-yp)**2)\n",
    "    R2cv = 1-RSScv/TSS\n",
    "    return R2cv,R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted R2:0.3503\n",
      "Crossvalidated R2:0.2882\n"
     ]
    }
   ],
   "source": [
    "R2cv,R2 = KfoldCVmultReg(D,D.weight,['age','smokeDummy','gestation','parity'])\n",
    "print(f\"Fitted R2:{R2:.4f}\")\n",
    "print(f\"Crossvalidated R2:{R2cv:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> R2 is 0.3503, and cv R2 is 0.2882. So overall, we are able to predict 28.82% of the variance "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2 ( / 15 pts)\n",
    "Using the R2cv from the 20-fold crossvalidation, determine the best predictive model for birthweight using the following candidate variables \n",
    "\n",
    "- age of mom\n",
    "- smoker (dummy coded) \n",
    "- gestation \n",
    "- parity \n",
    "\n",
    "Start with the R2cv for the full model (Question 2.1)and use **backwards** step-wise regression to find the best model (the model that increases R2cv the most). Show all steps of your selection procedure. Report the formula of your best model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted R2:0.3423\n",
      "Crossvalidated R2:0.2950\n"
     ]
    }
   ],
   "source": [
    "# Dropping Age\n",
    "R2cv,R2 = KfoldCVmultReg(D,D.weight,['smokeDummy','gestation','parity'])\n",
    "print(f\"Fitted R2:{R2:.4f}\")\n",
    "print(f\"Crossvalidated R2:{R2cv:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted R2:0.2244\n",
      "Crossvalidated R2:0.1626\n"
     ]
    }
   ],
   "source": [
    "# Dropping Smoke Dummy\n",
    "R2cv,R2 = KfoldCVmultReg(D,D.weight,['age','gestation','parity'])\n",
    "print(f\"Fitted R2:{R2:.4f}\")\n",
    "print(f\"Crossvalidated R2:{R2cv:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted R2:0.1917\n",
      "Crossvalidated R2:0.1282\n"
     ]
    }
   ],
   "source": [
    "# Dropping gestation\n",
    "R2cv,R2 = KfoldCVmultReg(D,D.weight,['age','smokeDummy','parity'])\n",
    "print(f\"Fitted R2:{R2:.4f}\")\n",
    "print(f\"Crossvalidated R2:{R2cv:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted R2:0.3503\n",
      "Crossvalidated R2:0.2936\n"
     ]
    }
   ],
   "source": [
    "# Dropping parity\n",
    "R2cv,R2 = KfoldCVmultReg(D,D.weight,['age','smokeDummy','gestation'])\n",
    "print(f\"Fitted R2:{R2:.4f}\")\n",
    "print(f\"Crossvalidated R2:{R2cv:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The R2cv increases most when dropping age as an explanatory variables. So we drop age and proceed with trying to drop another variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted R2:0.2216\n",
      "Crossvalidated R2:0.1757\n"
     ]
    }
   ],
   "source": [
    "# Dropping Smoke Dummy\n",
    "R2cv,R2 = KfoldCVmultReg(D,D.weight,['gestation','parity'])\n",
    "print(f\"Fitted R2:{R2:.4f}\")\n",
    "print(f\"Crossvalidated R2:{R2cv:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted R2:0.0067\n",
      "Crossvalidated R2:-0.0606\n"
     ]
    }
   ],
   "source": [
    "# Dropping gestation\n",
    "R2cv,R2 = KfoldCVmultReg(D,D.weight,['age','parity'])\n",
    "print(f\"Fitted R2:{R2:.4f}\")\n",
    "print(f\"Crossvalidated R2:{R2cv:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted R2:0.3403\n",
      "Crossvalidated R2:0.3014\n"
     ]
    }
   ],
   "source": [
    "# Dropping parity\n",
    "R2cv,R2 = KfoldCVmultReg(D,D.weight,['smokeDummy','gestation'])\n",
    "print(f\"Fitted R2:{R2:.4f}\")\n",
    "print(f\"Crossvalidated R2:{R2cv:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping Parity improves the cross-validated R2 most. \n",
    "Dropping either of these terms now decreases R2cv quite dramatically - so \n",
    "\n",
    "weight = b_0 + b_1 * smokeDummy + b_2 * gestation \n",
    "\n",
    "is our best model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Implement regularized regression to build a better predictive model (/35pts)\n",
    "In this task you will implement regularized regression to try to build a better predictive model for the birthweight of data. \n",
    "Like in Task 2, we will consider the following explanatory variables:\n",
    "\n",
    "- age of the mother \n",
    "- smoker (dummy coded) \n",
    "- birth occurred in Oakland? \n",
    "- gestation "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1: Z-standardize the regressors (/8pts)\n",
    "Write a function `zstandardize`, which takes as an input a pandas series or ndarray \n",
    "and returns a z-standardized version of the data \n",
    "\n",
    "Use the function to z-standardize the columns age,gestation,parity, and smokeDummy. \n",
    "\n",
    "Create new columns in the data frame called ageZ,gestationZ,parityZ, and smokeDummyZ.\n",
    "\n",
    "Check that the mean of the new variables in very close to and the std very close to 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 7.915890165577366e-16\n",
      "Std: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Z-standardize the data\n",
    "def zstandardize(d):\n",
    "    d = (d-d.mean())/d.std()\n",
    "    return d\n",
    "\n",
    "D['ageZ'] = zstandardize(D.age)\n",
    "D['gestationZ'] = zstandardize(D.gestation)\n",
    "D['parityZ'] = zstandardize(D.parity)\n",
    "D['smokeDummyZ'] = zstandardize(D.smokeDummy)\n",
    "\n",
    "print(f'Mean: {D.ageZ.mean()}')\n",
    "print(f'Std: {D.ageZ.std()}')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2 Implement Ridge regression (L2 regularized regression) (/17pts)\n",
    "\n",
    "To implement ridge regression you need to modify two functions, the most important being the loss function. \n",
    "Make a copy of the function `multRegLossRSS` from assigment 10. \n",
    "Rename it to `ridgeLoss`. Give the function an additional input parameter, namely alpha. Give this a default value of 1.0. \n",
    "\n",
    "Change the loss and the gradient to take into account the regularization. \n",
    "\n",
    "**Note that we are not regularizing the intercept regressor (b0)**\n",
    "\n",
    "Overall the function should take the following input arguments:\n",
    "\n",
    "    Args:\n",
    "        b (nd.array): Array of regression coefficients - first is intercept \n",
    "        D (pd.DataFrame): Pandas data frame with explanatory variables\n",
    "        y (ndarray): Dependent variable \n",
    "        xname (list): List of strings with names of explanatory variables\n",
    "        alpha (float): Regularization parameter \n",
    "\n",
    "    Returns:\n",
    "        loss: Current loss\n",
    "        grad: gradient of loss function in respect to parameters  \n",
    "\n",
    "Then make a copy of `multRegFit` from the last homework and rename it to `ridgeFit`. \n",
    "Again, you need to add an additional input parameter (alpha) to the function. \n",
    "Alpha needs to be passed to your loss function (`ridgeLoss`) when you call so.minimize: \n",
    "\n",
    "`so.minimize(ridgeLoss,b0,args=(D,y,xname,alpha),jac=True)`\n",
    "\n",
    "You need to take care when you calculate R2 of the fit - Since ridgeLoss does not return the \n",
    "residual-sum-of-squares, you need to use the appropriate function to calculate the RSS.\n",
    "\n",
    "To test your function:\n",
    "* Use it to fit a model that explains weight with the explanatory variables `['ageZ','smokeDummyZ','gestationZ','parityZ']`. Note: use the zstandardized version of the variables.  \n",
    "* Do the fit setting `alpha=0` and `alpha=8`, and report both R2 and the regression coefficients (b)\n",
    "* Compare the R2 between the two settings of alpha. Also compare to the one found for normal multiple regression (Question 2.1). What do you see and why?\n",
    "* Compare the regression weights (b) between the two settings of alpha. which regression weights changed and why? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridgeLoss(b,D,y,xname,alpha=1.0):\n",
    "    \"\"\"Loss function for Ridge regression\n",
    "\n",
    "    Args:\n",
    "        b (nd.array): Array of regression coefficients - first is intercept\n",
    "        D (pd.DataFrame): Pandas data frame with explanatory variables\n",
    "        y (ndarray): Dependent variable\n",
    "        xname (list): List of strings with names of explanatory variables\n",
    "        alpha (float): Ridge regression parameter\n",
    "    Returns:\n",
    "        rss: Current loss\n",
    "        grad: gradient of loss function in respect to parameters\n",
    "    \"\"\"\n",
    "    predY = multRegPredict(b,D,xname)\n",
    "    res = y-predY\n",
    "    rss = sum(res**2)+alpha*sum(b[1:]**2)\n",
    "    grad=np.zeros(len(b))\n",
    "    grad[0]=-2*np.sum(res)\n",
    "    for i in range(len(xname)):\n",
    "        grad[i+1]=-2*np.sum(D[xname[i]]*res)+2*alpha*b[i+1]\n",
    "    return (rss,grad)\n",
    "\n",
    "def ridgeFit(D,y,xname,figure=0,b0=[],alpha=1.0):\n",
    "    \"\"\"Fits a multiple regression loss function\n",
    "\n",
    "    Args:\n",
    "        D (pd.DataFrame): Pandas data frame with explanatory variables\n",
    "        y (ndarray): Dependent variable\n",
    "        xname (list): List of strings with names of explanatory variables\n",
    "        figure (int): Plot figure? Defaults to 0.\n",
    "        b0 (np.ndarray). Initial guess for the parameter vector\n",
    "        alpha (float): Ridge regression parameter\n",
    "    Returns:\n",
    "        R2: Fitted R2 value\n",
    "        b: Fitted\n",
    "    \"\"\"\n",
    "    k=len(xname)+1\n",
    "    if (len(b0)!=k):\n",
    "        b0=np.zeros((k,))\n",
    "    RES = so.minimize(ridgeLoss,b0,args=(D,y,xname,alpha),jac=True)\n",
    "    b=RES.x # Results\n",
    "    res = y-np.mean(y)\n",
    "    TSS = sum(res**2)\n",
    "    RSS,deriv = multRegLossRSS(b,D,y,xname)\n",
    "    R2 = 1-RSS/TSS\n",
    "    if (k==2 and figure==1):\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "        ax.scatter(D[xname[0]],y)\n",
    "        xRange=[min(D[xname[0]]),max(D[xname[0]])]\n",
    "        xp=np.arange(xRange[0],xRange[1],(xRange[1]-xRange[0])/50)\n",
    "        yp=b[0]+b[1]*xp\n",
    "        ax.plot(xp,yp,'r-')\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "    return (R2,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha=0\n",
      "R2: 0.3503\n",
      "alpha=8\n",
      "R2: 0.3488\n"
     ]
    }
   ],
   "source": [
    "R2_0,b_0 = ridgeFit(D,D.weight,['ageZ','smokeDummyZ','gestationZ','parityZ'],alpha=0.0)\n",
    "R2_1,b_1 = ridgeFit(D,D.weight,['ageZ','smokeDummyZ','gestationZ','parityZ'],alpha=8.0)\n",
    "\n",
    "print('alpha=0')\n",
    "print(f'R2: {R2_0:.4f}')\n",
    "\n",
    "print('alpha=8')\n",
    "print(f'R2: {R2_1:.4f}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitted R2 value reduces when we add regularization to the model. The R2 for `alpha=0` is identical to normal regression - because we set the regularization to zero, so it has no effect. *BTW: this is also a good test if your functions work correctly!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha=0\n",
      "[ 7.495e+00 -1.040e-01 -3.830e-01  4.340e-01  4.000e-03]\n",
      "alpha=8\n",
      "[ 7.495e+00 -9.400e-02 -3.580e-01  4.050e-01  3.000e-03]\n"
     ]
    }
   ],
   "source": [
    "print('alpha=0')\n",
    "print(b_0.round(3))\n",
    "print('alpha=8')\n",
    "print(b_1.round(3))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intercept regressor (`b[0]`) did not change, as it was not regularized. However, all other regression coefficients are now closer to zero. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.3: Crossvalidate Ridge regression (10pts)\n",
    "Copy your function `KfoldCVmultReg` from Question 2.1, rename it to `KfoldCVridge`, and modify it to work with Ridge regression. \n",
    "That means it needs to take an additional input parameter `alpha` that it passes on to the fitting function. \n",
    "\n",
    "To calculate the R2 and R2cv for the model of `weight` using the explanatory variables `['ageZ','smokeDummyZ','gestationZ','parityZ']`. Like in question 3.2, use the standardized versions of the variables and try both the setting `alpha=0` and `alpha=8`. \n",
    "\n",
    "How to the R2 and R2cv values compare between the two settings of alpha? Do you get better predictive performance than the reduced model that you found using feature selection (Question 2.2)?\n",
    "\n",
    "\n",
    "*Note: If you want, play a bit with the regularization parameter to see if you can find a better setting. What happens when you make `alpha` very large (i.e. 1000)? (this is optional, but educational)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KfoldCVridge(D,y,xname,K=20,alpha=1.0):\n",
    "    \"\"\"K-fold Crossvalidation for multiple regression\"\"\"\n",
    "    N = len(y) #Number of observations\n",
    "    yp= np.zeros(N)\n",
    "\n",
    "    # Make an index vector with K folds\n",
    "    ind = np.arange(N)\n",
    "    ind = np.floor(ind/N*K)\n",
    "\n",
    "    # Get overall model fit\n",
    "    R2,b_all=ridgeFit(D,y,xname,alpha=alpha)\n",
    "\n",
    "    # Loop over the crossvalidation folds\n",
    "    for i in range(K):\n",
    "        r,b=ridgeFit(D[ind!=i],y[ind!=i],xname,b0=b_all,alpha=alpha)\n",
    "        yp[ind==i]=multRegPredict(b,D[ind==i],xname)\n",
    "\n",
    "    # Calculate crossvalidated model fit\n",
    "    TSS  = sum((y-y.mean())**2)\n",
    "    RSScv = sum((y-yp)**2)\n",
    "    R2cv = 1-RSScv/TSS\n",
    "    return R2cv,R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted R2:0.3503\n",
      "Crossvalidated R2:0.2882\n"
     ]
    }
   ],
   "source": [
    "R2cv,R2 = KfoldCVridge(D,D.weight,['ageZ','smokeDummyZ','gestationZ','parityZ'],alpha=0)\n",
    "print(f\"Fitted R2:{R2:.4f}\")\n",
    "print(f\"Crossvalidated R2:{R2cv:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted R2:0.3487\n",
      "Crossvalidated R2:0.2950\n"
     ]
    }
   ],
   "source": [
    "R2cv,R2 = KfoldCVridge(D,D.weight,['ageZ','smokeDummyZ','gestationZ'],alpha=8)\n",
    "print(f\"Fitted R2:{R2:.4f}\")\n",
    "print(f\"Crossvalidated R2:{R2cv:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated in Question 3.2. the fitted R2 becomes smaller for higher regularization parameter, because we do not minimize RSS anymore. \n",
    "\n",
    "In contrast, cross-validated R2 increases when regularizing the model. However the regularized model is not quite as good as the one that we found in question 2.2 using backwards stepwise regression. \n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Use logistic regression to predict complications \n",
    "In this task you will create and test a logistic regression model that predicts the presence of a complication in the first three month (0: no complication, 1: complication). \n",
    "\n",
    "** Task 4 of the Homework does not have to be handed in and will not be graded! It is only added here to provide additional preparation and practice for you for the final. So if you are short on time, leave these questions open and solve them when you practice for the final.**\n",
    "\n",
    "### Question 4.1: Improving your logistic regression model code\n",
    "Improve your code for logisitic regression in two ways: \n",
    "\n",
    "1. prevent log(0) errors by making sure that your predicted value never is smaller than 1e-20 or larger than 1-1e-20. (tip you can use the numpy function `clip`)\n",
    "\n",
    "2. Let logisticRegFit take an additional input parameter, telling it whether it should plot a figure or not (figure=1) \n",
    "\n",
    "3. Let logisticRegFit take an additional input parameter, specifying the starting value for the parameters (b_init=[]). If b_init is empty, the function should start with a vector off all zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticRegPredict(b,D,xname):\n",
    "    yp=np.ones(len(D.index))*b[0]       # Start out with the intercept\n",
    "    for i in range(len(xname)):\n",
    "        yp=yp+D[xname[i]]*b[i+1]        # Add the prediction of each regressor seperately\n",
    "    p = np.exp(yp)/(1+np.exp(yp))\n",
    "    p = p.clip(1e-12,1-(1e-12))\n",
    "    return p\n",
    "\n",
    "def logisticRegLoss(b,D,y,xname):\n",
    "    p = logisticRegPredict(b,D,xname)\n",
    "    cost = -y*np.log(p)-(1-y)*np.log(1-p)\n",
    "    N=len(xname)\n",
    "    grad=np.zeros(N+1)\n",
    "    res = y-p\n",
    "    grad[0]=-sum(res)\n",
    "    for i in range(N):\n",
    "        grad[i+1]=-np.sum(D[xname[i]]*res)         # Add each regressor\n",
    "    return (cost.sum(),grad)\n",
    "\n",
    "def logisticRegFit(D,y,xname,figure=0,b_init=[]):\n",
    "    k=len(xname)+1\n",
    "    if (len(b_init)!=k):\n",
    "        b_init=np.zeros(k)\n",
    "    RES = so.minimize(logisticRegLoss,b_init,args=(D,y,xname),jac=True)\n",
    "    b = RES.x\n",
    "    ll = -RES.fun # Negative function value is the log likelihood\n",
    "    p = logisticRegPredict(b,D,xname)\n",
    "    if (k==2 & figure==1):\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "        ax.scatter(D[xname[0]],y)\n",
    "        xRange=[min(D[xname[0]]),max(D[xname[0]])]\n",
    "        xp=np.arange(xRange[0],xRange[1],(xRange[1]-xRange[0])/50)\n",
    "        yp=b[0]+b[1]*xp\n",
    "        pp=np.exp(yp)/(1+np.exp(yp))\n",
    "        ax.plot(xp,pp,'r-')\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "    return (ll,b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2: Crossvalidation of logistic models\n",
    "Modify the KfoldCVmultReg function to make it work for logistic regression. As before, use K-fold crossvalidation. The main changes are that \n",
    "- you need to use logisticRegFit and logisticRegPredict as fitfcn and predictfcn respectively. \n",
    "- To save time, initialize each optimization with from the parameters that you found on the entire data by setting `b_init` \n",
    "- instead of the crossvalidated R2, your function should return the crossvalidated log-likelihood and non-crossvalidated log-likelihood. \n",
    "\n",
    "Using your function, calculate the the difference in crossvalidated log-likelihood for the model that predicts complications with an intercept only (b0) and a model that predicts complications with an intercept and smokeDummy. \n",
    "From the difference, report the Bayes-Factor between the two models. What do you conclude? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KfoldCVlogisticReg(D,y,xname,K=20,fitfcn=logisticRegFit,predictfcn=logisticRegPredict):\n",
    "    N = len(y) #Number of observations\n",
    "    yp= np.zeros(N)\n",
    "    ind = np.arange(N)\n",
    "    ind = np.floor(ind/N*K)\n",
    "\n",
    "    # Get overall model fit\n",
    "    LL,b_all=fitfcn(D,y,xname,figure=0)\n",
    "\n",
    "    # Loop over the crossvalidation folds\n",
    "    for i in range(K):\n",
    "        r,b=fitfcn(D[ind!=i],y[ind!=i],xname,b_init=b_all,figure=0)\n",
    "        yp[ind==i]=predictfcn(b,D[ind==i],xname)\n",
    "    LLcv = sum(y*np.log(yp)+(1-y)*np.log(1-yp))\n",
    "    return LLcv,LL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood difference between Smoke and intercept model:1.8248\n",
      "Bayes Factor: 6.201\n"
     ]
    }
   ],
   "source": [
    "llcv0,ll0=KfoldCVlogisticReg(D,D.complication,[])\n",
    "llcv1,ll1=KfoldCVlogisticReg(D,D.complication,['smokeDummy'])\n",
    "print(f'Log-likelihood difference between Smoke and intercept model:{llcv1-llcv0:.4f}')\n",
    "print(f'Bayes Factor: {np.exp(llcv1-llcv0):.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There is positive evidence that smoking leads to increased chance of complications in the first 3 month.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.3:\n",
    "Compare the model that uses only intercept, smoking as explanatory variable to one that uses: \n",
    "\n",
    " * intercept, smoking, weight \n",
    " * intercept, smoking, age \n",
    " * intercept, smoking, weight, age \n",
    " \n",
    "Report the cross-validated Log-likelihood for each model. Which one is the best model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding weight to baseline mode: 1.751\n"
     ]
    }
   ],
   "source": [
    "llcv2,ll2=KfoldCVlogisticReg(D,D.complication,['smokeDummy','weight'])\n",
    "print(f'Adding weight to baseline mode: {llcv2-llcv1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding age to baseline mode: -0.976\n"
     ]
    }
   ],
   "source": [
    "llcv3,ll3=KfoldCVlogisticReg(D,D.complication,['smokeDummy','age'])\n",
    "print(f'Adding age to baseline mode: {llcv3-llcv1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding age and weight to baseline mode: 0.699\n"
     ]
    }
   ],
   "source": [
    "llcv4,ll4=KfoldCVlogisticReg(D,D.complication,['smokeDummy','weight','age'])\n",
    "print(f'Adding age and weight to baseline mode: {llcv4-llcv1:.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> According to the crossvalidated likelihood, the model with smoking, weight is the best model. However, there is no positive evidence (in terms of a Bayes factor) that adding age into the model increases the prediction. The most complex model (smoking, weight, age) is not better than the model with only smoking and weight. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.4 : \n",
    "In the model (['smokeDummy','weight']), how do each of the explanatory variables contribute to the chance of complication? That is, for each variable, would an increase an the variable lead to an increased or decreased probability of a complication? \n",
    "\n",
    "What reduction in birth weight causes the same amount of risk of complication as having a smoking mother?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 3.764\n",
      "Smoke: 0.611\n",
      "Weight: -0.719\n"
     ]
    }
   ],
   "source": [
    "ll,b=logisticRegFit(D,D.complication,['smokeDummy','weight'])\n",
    "print(f'Intercept: {b[0]:.3f}')\n",
    "print(f'Smoke: {b[1]:.3f}')\n",
    "print(f'Weight: {b[2]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of slopes smoking / pounds: -0.850\n"
     ]
    }
   ],
   "source": [
    "print(f'Ratio of slopes smoking / pounds: {b[1]/b[2]:.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Smoking increases the chances of complication. Lower weight also increases the chances of complications. Smoking adds about as much risk as a lower birthweight of 0.85 pounds."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
