{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 11: Model Selection, Regularized regression and final practice\n",
    "## Learning goals \n",
    "This homework includes some repeated task and variations on model fitting, and model comparison, which should prepare you optimally for the final. \n",
    "The homework also introduces z-standardization and regularized (L2) regression. \n",
    "Try to solve the task in the homework independently, and prepare a cheat-sheet and a file with useful functions, such that you can complete this homework in 3 hrs or less. \n",
    "\n",
    "## Data set \n",
    "The file kaiser.csv contains a subset of data from the Child Health and Development Studies, which investigate a range of topics. One study considered all pregnancies between 1960 and 1967 among women in the Kaiser Foundation Health Plan in the San Francisco East Bay area. Here, we look at the predictor of birth weight of babies, measured in pounds, as well as the occurrence of complications in the first 3 month. \n",
    "\n",
    "The data frame stored in kaiser.csv contains the variables: \n",
    "- age:          Age of the mom at time of birth\n",
    "- smoke:        Is the mom a smoker / non-smoker? \n",
    "- hospital:     Which hospital was the birth at? Oakland, SanFrancisco, WalnutCreek, SanJose, and Richmond.\n",
    "- gestation:    Gestation period (length of pregnancy) [days]\n",
    "- parity:       1: child the first born 0: Child has older siblings \n",
    "- weight:       Weight of the baby [pounds].  \n",
    "- complication: Was there a complication within the first 3 month of pregnancy (0: No 1:Yes) \n",
    "\n",
    "## Preliminaries\n",
    "Set up the environment by importing pandas, seaborn, numpy, scipy.optimize and matplotlib. \n",
    "Then add your multiple regression functions (multRegPredict,multRegLossRSS, multRegFit) from the last homeworks. \n",
    "\n",
    "To make it easier - we have done these things already! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as so\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multRegPredict(b,D,xname):\n",
    "    \"\"\"Prediction function for multiple regression\n",
    "\n",
    "    Args:\n",
    "        b (nd.array): Array of regression coefficients - first is intercept\n",
    "        D (pd.DataFrame): Pandas data frame with explanatory variables\n",
    "        xname (list): List of strings with names of explanatory variables\n",
    "\n",
    "    Returns:\n",
    "        yp (nd.array): Predicted y - values\n",
    "    \"\"\"\n",
    "    yp=np.ones(len(D.index))*b[0]        # Intercept\n",
    "    for i in range(len(xname)):\n",
    "        yp=yp+D[xname[i]]*b[i+1]         # Add each regression value\n",
    "    return yp\n",
    "\n",
    "def multRegLossRSS(b,D,y,xname):\n",
    "    \"\"\"Loss function for OLS multiple regression\n",
    "\n",
    "    Args:\n",
    "        b (nd.array): Array of regression coefficients - first is intercept\n",
    "        D (pd.DataFrame): Pandas data frame with explanatory variables\n",
    "        y (ndarray): Dependent variable\n",
    "        xname (list): List of strings with names of explanatory variables\n",
    "\n",
    "    Returns:\n",
    "        rss: Current loss\n",
    "        grad: gradient of loss function in respect to parameters\n",
    "    \"\"\"\n",
    "    predY = multRegPredict(b,D,xname)\n",
    "    res = y-predY\n",
    "    rss = sum(res**2)\n",
    "    grad=np.zeros(len(b))\n",
    "    grad[0]=-2*np.sum(res)\n",
    "    for i in range(len(xname)):\n",
    "        grad[i+1]=-2*np.sum(D[xname[i]]*res)\n",
    "    return (rss,grad)\n",
    "\n",
    "def multRegFit(D,y,xname=[],figure=0,b0=[]):\n",
    "    \"\"\"Fits a multiple regression loss function\n",
    "\n",
    "    Args:\n",
    "        D (pd.DataFrame): Pandas data frame with explanatory variables\n",
    "        y (ndarray): Dependent variable\n",
    "        xname (list): List of strings with names of explanatory variables\n",
    "        figure (int): Plot figure? Defaults to 0.\n",
    "        b0 (np.ndarray). Initial guess for the parameter vector\n",
    "\n",
    "    Returns:\n",
    "        R2: Fitted R2 value\n",
    "        b: Fitted\n",
    "    \"\"\"\n",
    "    k=len(xname)+1\n",
    "    if (len(b0)!=k):\n",
    "        b0=np.zeros((k,))\n",
    "    RES = so.minimize(multRegLossRSS,b0,args=(D,y,xname),jac=True)\n",
    "    b=RES.x # Results\n",
    "    res = y-np.mean(y)\n",
    "    TSS = sum(res**2)\n",
    "    RSS,deriv = multRegLossRSS(b,D,y,xname)\n",
    "    R2 = 1-RSS/TSS\n",
    "    if (k==2 and figure==1):\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "        ax.scatter(D[xname[0]],y)\n",
    "        xRange=[min(D[xname[0]]),max(D[xname[0]])]\n",
    "        xp=np.arange(xRange[0],xRange[1],(xRange[1]-xRange[0])/50)\n",
    "        yp=b[0]+b[1]*xp\n",
    "        ax.plot(xp,yp,'r-')\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "    return (R2,b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Multiple regression with discrete variables ( / 30 pts)\n",
    "### Question 1.1 ( / 10pt)\n",
    "Create a dummy variable for Smoker / Non-smoker. Set the value for “Smoker” to 1 and for “Non-smoker” to 0. Estimate a regression model with the dummy variable as a regressor *and birth weight as the response variable.* Use the RSS as the loss function. \n",
    "\n",
    "Report the value of the intercept and slope. \n",
    "\n",
    "Written answer: What does the intercept and slope value indicate? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept (b0): 7.688278175592858\n",
      "Slope (b1): -1.2083133727774822\n",
      "R-squared: 0.1773088876372123\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('kaiser.csv')\n",
    "df['smokeDummy'] = (df['smoke'] == 'smoker').astype(int)\n",
    "R2, b = multRegFit(df, df['weight'], ['smokeDummy'])\n",
    "\n",
    "print(f\"Intercept (b0): {b[0]}\")\n",
    "print(f\"Slope (b1): {b[1]}\")\n",
    "print(f\"R-squared: {R2}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intertecpt is the average birth weight for non smokers. The slope indicates the difference in birth weight between smokers and non smokers. In this case since slope is negative. There is a negative correlation between smoker and birth weight"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2 ( / 8pt)\n",
    "Make a boxplot of hospital on the x-axis and birthweight on the y-axis (see HW2 for an example). Which hospital has the lowest overall birth weight?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Birth Weight')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0kAAAINCAYAAADrxzSOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABDHUlEQVR4nO3de1xUdeL/8TcgMKKAQQlqzCAq3hLTvEdoZmVXbbUstYvWUmq13ipNN7Qks4utltpirbpl1y27Wtmuptma6yUjSc0UGfJuKlgKJnx+f/RjvmcCDQrmDPB6Ph7z0DnnzDlv4MiZt2fO5wQYY4wAAAAAAJKkQLsDAAAAAIA/oSQBAAAAgAUlCQAAAAAsKEkAAAAAYEFJAgAAAAALShIAAAAAWFCSAAAAAMCCkgQAAAAAFnXsDlDViouLtWfPHoWHhysgIMDuOAAAAABsYozRsWPH1LhxYwUGnv58UY0vSXv27FFcXJzdMQAAAAD4idzcXJ177rmnnV/jS1J4eLikX74RERERNqcBAAAAYJf8/HzFxcV5OsLp1PiSVPIRu4iICEoSAAAAgN+8DIeBGwAAAADAgpIEAAAAABaUJAAAAACwoCQBAAAAgAUlCQAAAAAsKEkAAAAAYEFJAgAAAAALShIAAAAAWFCSAAAAAMCCkgQAAAAAFpQkAAAAALCgJAEAAACABSUJAAAAACwoSQAAAABgQUkCAAAAAAtKEgAAAABYUJIAAAAAwKKO3QEAAAAqU0FBgdxut90xbON0OuVwOOyOAVRrlCQAAFCjuN1upaam2h3DNhkZGUpMTLQ7BlCtUZIAAECN4nQ6lZGRYcu2c3JylJ6erkmTJsnlctmSwel02rJdoCahJAEAgBrF4XDYfibF5XLZngHA78fADQAAAABgYXtJ2rFjh/7+979r0KBBeuaZZ0rNf/nll9WsWTPVq1dPAwcO1A8//GBDSgAAAAC1he0lacCAAfr444/13nvvlSpAK1euVGpqqmbNmqUtW7aoqKhIgwcPtikpAAAAgNrA9muSvvzySwUEBOjss88uNe/JJ5/UsGHDdPXVV0uSnnvuOTVp0kSbNm3S+eef7+OkAAAAAGoD288kBQQElDn9+PHjWrp0qS6++GLPtJiYGLVu3VpvvPGGr+IBAAAAqGVsP5N0Ort371ZxcbHi4uK8psfFxSknJ+e0ryssLFRhYaHneX5+fpVlBAAAAFDz2H4m6XQOHTokSaXuGO1wODzzyjJ9+nRFRkZ6Hr8uWQAAAABwJn5bks455xxJUkFBgdf0EydOeOaVZeLEicrLy/M8cnNzqzQnAAAAgJrFb0tSkyZNFBQUpO+//95rutvtPuOdpENDQxUREeH1AAAAAIDy8tuSVLduXV111VVasWKFZ9revXu1detWDRw40MZkAAAAAGoy2wduyM/PV3FxsYwxKigo0NGjR+VwOORwODRu3DhdeeWVuvLKK9W6dWvdfffd6t27tzp06GB3bAAAAAA1lO0lKSkpyTNa3YwZMzRjxgylpaVpypQpSklJ0XPPPae77rpLBw8e1JVXXqmFCxfaGxgAAADQL9fOu91uu2PYxul0lhpkraawvSTt2rXrjPOHDh2qoUOH+iYMAAAAUE5ut1upqal2x7BNRkaGEhMT7Y5RJWwvSQAAAEB15HQ6lZGRYcu2c3JylJ6erkmTJsnlctmS4UyDqVV3lCQAAADgd3A4HLafSXG5XLZnqIn8dnQ7AAAAALADJQkAAAAALChJAAAAAGBBSQIAAAAAC0oSAAAAAFhQkgAAAADAgpIEAAAAABaUJAAAAACwoCQBAAAAgAUlCQAAAAAsKEkAAAAAYEFJAgAAAAALShIAAAAAWFCSAAAAAMCCkgQAAAAAFpQkAAAAALCgJAEAAACABSUJAAAAACwoSQAAAABgQUkCAAAAAAtKEgAAAABYUJIAAAAAwIKSBAAAAAAWlCQAAAAAsKAkAQAAAIAFJQkAAAAALChJAAAAAGBRx+4A8C9FRUXKzMzU4cOHFRUVpaSkJAUFBdkdCwAAAPAZShI8Vq1apblz52rfvn2eabGxsRo5cqRSUlJsTAYAAAD4Dh+3g6RfClJaWpoSEhI0Z84cLV26VHPmzFFCQoLS0tK0atUquyMCAAAAPkFJgoqKijR37lx1795d06ZNU9u2bRUWFqa2bdtq2rRp6t69u+bNm6eioiK7owIAAABVjpIEZWZmat++fRoyZIgCA713icDAQA0ZMkR79+5VZmamTQkBAAAA36EkQYcPH5YkNW3atMz5JdNLlgMAAABqMkoSFBUVJUnKzs4uc37J9JLlAAAAgJqMkgQlJSUpNjZWixcvVnFxsde84uJiLV68WI0aNVJSUpJNCQEAAADfoSRBQUFBGjlypNasWaPJkycrKytLx48fV1ZWliZPnqw1a9ZoxIgR3C8JAAAAtQL3SYIkKSUlRVOnTtXcuXM1atQoz/RGjRpp6tSp3CcJAAAAtQYlCR4pKSm68MILlZmZqcOHDysqKkpJSUmcQQIAAECtQkmCl6CgIHXo0MHuGAAAAIBtuCYJAAAAACwoSQAAAABgQUkCAAAAAAuuSYKXoqIiBm4AAABArUZJgseqVas0d+5c7du3zzMtNjZWI0eOZAhwAAAA1Bp83A6SfilIaWlpSkhI0Jw5c7R06VLNmTNHCQkJSktL06pVq+yOCAAAAPgEJQkqKirS3Llz1b17d02bNk1t27ZVWFiY2rZtq2nTpql79+6aN2+eioqK7I4KAAAAVDlKEpSZmal9+/ZpyJAhCgz03iUCAwM1ZMgQ7d27V5mZmTYlBAAAAHyHkgQdPnxYktS0adMy55dML1kOAAAAqMkoSVBUVJQkKTs7u8z5JdNLlgMAAABqMkoSlJSUpNjYWC1evFjFxcVe84qLi7V48WI1atRISUlJNiUEAAAAfIeSBAUFBWnkyJFas2aNJk+erKysLB0/flxZWVmaPHmy1qxZoxEjRnC/JAAAANQK3CcJkqSUlBRNnTpVc+fO1ahRozzTGzVqpKlTp3KfJAAAANQalCR4pKSk6MILL1RmZqYOHz6sqKgoJSUlcQYJAAAAtQolCV6CgoLUoUMHu2MAAAAAtuGaJAAAAACwoCQBAAAAgAUlCQAAAAAsKEkAAAAAYMHADQAAnygoKJDb7bY7hm2cTqccDofdMQAA5UBJAmo53rjyxtVX3G63UlNT7Y5hm4yMDCUmJtodAwBQDpQkoJbjjStvXH3F6XQqIyPDlm3n5OQoPT1dkyZNksvlsiWD0+m0ZbsAgIqjJAG1HG9ceePqKw6Hw/ZC6nK5bM8AAPB/lCSgluONKwAAgDdGtwMAAAAAC0oSAAAAAFhQkgAAAADAwu9LUkZGhuLj41W3bl316dNH3333nd2RAAAAANRgfl2S3nnnHT3wwAOaN2+esrKy5HK5dNlll+nEiRN2RwMAAABQQ/l1SXr00Ud155136oorrlBCQoLmzZungoICvfTSS3ZHAwAAAFBD+XVJ+uabb9S0aVPP85CQEHXp0kUrVqywMRUAAACAmsyv75MUHR2t7Oxsr2nHjx/XDz/8cNrXFBYWqrCw0PM8Pz+/yvIBAAAAqHn8+kzSLbfcohdeeEFffvmliouLtWjRIq1cuVLFxcWnfc306dMVGRnpecTFxfkwMQAAAIDqzq9L0qRJk3Tdddepa9euCgsL0+rVq9W7d2+dffbZp33NxIkTlZeX53nk5ub6MDEAAACA6s6vP24XGhqqjIwMzZ49W4WFhYqMjFTbtm11ww03nPE1oaGhPkwJAAAAoCbx65JUwuFwyOFwaOPGjdq6datuvPFGuyMBAAAAqKH8uiStW7dOJ06cUGJiorKzs3XHHXdo7Nixatmypd3RAAAAANRQfl2SsrOzdf/992vv3r1q0qSJRowYoXHjxtkdCwAAAEAN5tcl6YYbbjjj9UcAAAAAUNn8enQ7AAAAAPA1ShIAAAAAWFCSAAAAAMCCkgQAAAAAFpQkAAAAALCgJAEAAACABSUJAAAAACwoSQAAAABgQUkCAAAAAAtKEgAAAABYUJIAAAAAwIKSBAAAAAAWlCQAAAAAsKAkAQAAAIAFJQkAAAAALChJAAAAAGBBSQIAAAAAC0oSAAAAAFhQkgAAAADAgpIEAAAAABaUJAAAAACwoCQBAAAAgAUlCQAAAAAsKEkAAAAAYEFJAgAAAAALShIAAAAAWFCSAAAAAMCCkgQAAAAAFpQkAAAAALCgJAEAAACABSUJAAAAACwoSQAAAABgQUkCAAAAAAtKEgAAAABYUJIAAAAAwIKSBAAAAAAWlCQAAAAAsKAkAQAAAIAFJQkAAAAALChJAAAAAGBBSQIAAAAAC0oSAAAAAFhQkgAAAADAgpIEAAAAABZ17A4AAABqpv379ysvL8/uGD6Vk5Pj9WdtEhkZqZiYGLtjAJWCkgQAACrd/v37NfTmW/TzyUK7o9giPT3d7gg+FxwSqpde/CdFCTUCJQkAAFS6vLw8/XyyUCcSeqrYEWl3HFSxwII8aedK5eXlUZJQI1CSAABAlSl2RKq43tl2xwCACqEk+amCggK53W67Y9jG6XTK4XDYHQMAAAC1ECXJT7ndbqWmptodwzYZGRlKTEy0OwYAAABqIUqSn3I6ncrIyLBl2zk5OUpPT9ekSZPkcrlsyeB0Om3ZLgAAAEBJ8lMOh8P2Mykul8v2DAAAAICvUZIAP8H9RGoX7icCAID/oiQBfoD7iXA/EQAA4D8oSYAf4H4itQv3EwEAwL9RkgA/wv1EAAAA7BdodwAAAAAA8CeUJAAAAACw4ON2AFDLMJJi7cJIigBQcZQkAKhFGEmRkRSBmoj//KldfPGfP5QkAKhFGEmxdmEkRdQG/OcP//lTFShJAFALMZIigJqC//ypXXz1nz+UJAAAAFR7/OcPKhOj2wEAAACABSUJAAAAACz4uN1vYLSU2oWhcgEAAEBJOgNGS2G0FAAAANQ+lKQzYLSU2oWhcgEAACBVk5KUlZWl0aNHa+3atapXr56GDh2qxx57TEFBQT7ZPqOlAAAAALWH3w/cUFBQoL59+6pHjx7aunWrXnrpJb3wwguaN2+e3dEAAAAA1EB+fybpm2++0ffff6/77rtP9evXV+PGjXXJJZfo22+/tTsaAAAAgBrI788kxcXFKSQkRM8++6wk6ccff9TatWs1YMAAm5MBAAAAqIn8/kzSOeeco7lz5yo1NVWbN2/WgQMHNHnyZPXs2bPM5QsLC1VY+H+j0eXn5/sqKgAAAIAawO/PJElSYGCgWrZsqaysLH322Wc6cuTIaZedPn26IiMjPY+4uDgfJgUAAABQ3VW4JPXu3VvHjx8vNf3w4cO69dZbKyWU1aeffqqxY8fqo48+0rp16zR+/HhNmDBBjz76aJnLT5w4UXl5eZ5Hbm5upWcCAAAAUHNVuCStXLlSRUVFpaavX79eS5YsqZRQVhkZGbrkkkvkdDpVp04dPfLII7r99ts1e/bsMpcPDQ1VRESE1wMAAAAAyqvc1yTNmjVLs2bNkiS1a9dOgYH/169OnTqlPXv2aNCgQZUesLCwsNTH61wul9f2AQAAAKCylLskde/eXUeOHNHDDz+sm266SQ6HwzMvKChIzZs318CBAys94J/+9CcNHTpUs2fP1oABA/TNN9/o2Wef1fDhwyt9WwAAAABQ7pLUpUsXdenSRU2bNtXgwYMVHBxclbk8hgwZop9++kmzZs3ShAkT1LhxY/3lL3/R/fff75PtAwAAAKhdKjwEeFUMzvBbUlNTlZqa6vPtAgAAAKh9KlySsrKy9MADD2jbtm1lDuCwc+fOSgkGAAAAAHaocEkaNGiQoqKiNGjQIJ995A4AAAAAfKXCJcntdmvZsmVq3LhxVeQBAAAAAFtVeBzt6667rkruhwQAAAAA/qBcZ5JWrVrl+fuAAQN077336scff1T37t1LLZuSklJ56QAAAADAx8pVknr16qWAgAAZYzzTJk6cWGq5gICAMgdzAAAAAIDqolwlKTs7u6pzAAAAAIBfKFdJcrlcVZ0DAAAAAPxChUe3W7p0aalpAQEBCg4OVv369ZWQkKCGDRtWSjgAAAAA8LUKl6Srr7661PVJkjzTAgICdMkll+hf//qXIiIiKi0oAAAAAPhChYcAnz17ti677DLt3LlT2dnZys7O1tq1a3XBBRfovffe04cffqgDBw7ogQceqIq8AAAAAFClKnwmadasWXrvvfcUHx/vmeZyufToo4/qr3/9q9asWaPw8HANHjy4MnMCAAAAgE9U+EzSnj17VFBQUGp6ixYt9NVXX0mSEhIStG/fvj+eDgAAAAB8rMIl6eqrr9agQYO0ZMkS7d69W0eOHNHGjRt19913q127dpKknTt36pxzzqn0sAAAAABQ1Sr8cbuMjAz9+c9/1sCBA72mt23bVq+++qokaf/+/Ro+fHjlJAQAAAAAH6pwSYqMjNTrr7+unJwcZWVl6cSJE2ratKk6duzoWaZ///7q379/ZeYEAAAAAJ+ocEkq4XK5uMksAAAAgBqnXCXp4Ycf1oQJExQSEqKHH374jMs+9NBDlRIMAAAAAOxQrpK0YMECjR07ViEhIVqwYMFplwsICKAkAQAAAKjWylWSsrOzy/x7bRF44qjdEeAD/JwBAAAg/YFrkmqTutmr7I4AAAAAwEd+V0lavny5XnvtNeXm5mrhwoVq2LCh3n77bX311VdKS0ur7Iy2O9E0RcV1G9gdA1Us8MRRCjEAAAAqXpKefvpp/fWvf1WfPn20fPlyFRQUSJKaNWumcePG1ciSVFy3gYrrnW13DAAAAAA+UOGSNGvWLH3wwQfq2bOnzjrrLM/06Oho7d69u1LDAQAAAICvBVb0BUeOHFF8fHyp6cuXL1fjxo0rIxMAAAAA2KbCZ5JuuOEG3XHHHXrkkUckSTt37tRbb72lqVOnasKECZUeEKhNGGGvduDnDACAf6twSZo9e7b+8pe/6KKLLlJRUZF69+6t4OBg3XvvvXrggQeqIiNQazBwBAAAgP0qXJLq1q2rjIwMpaena8uWLTp58qQ6duyoqKioqsgH1CqMpFg7MJIiAFQ+ztLXDr76OZerJF111VXq3bu3Lr74YnXs2FGSdM455+icc86p0nBAbcNIigAA/D785xMqU7lK0qWXXqrly5frkUceUVBQkFJSUjyl6bzzzqvqjAAAAMAZ8WmM2sFXn8YoV0kaPXq0Ro8ereLiYq1bt04rVqzQe++9pwkTJqh+/frq1auXpzQlJiZWdWYAAADAC5/GQGWq0DVJgYGB6tq1q7p27aoJEybo0KFDevbZZzVnzhz961//kiQVFRVVSVAAAAAA8IVyl6RTp05p06ZNWrdunf73v//pf//7n7Zt26bi4mI1btxY1157rTp37lyVWQEAlYQLnGsHfs4A8PuUqyR169ZNX331lU6ePKmzzjpLF1xwga677jp17txZnTt35iayAFDNcIEzAACnV+4zScYY1atXT3379lWPHj3UrVs3tW/fXkFBQVWZDwBQBbjAuXZguHkA+H3KVZK++OILnTx5Ul9++aXWrl2r1atX68knn9SBAwfUsWNHdevWzfPgrBIA+D8ucAYA4PTKfSYpJCTEM2jDvffeK0k6cOCAFi1apGeeeUZPPfWUJAZuAAAAAFC9VWh0u+LiYm3cuFGfffaZVq9erc8//1wHDhyQw+FQcnKykpOTqyonAAAAAPhEuUrSlClTtHr1aq1du1Y//fSToqOj1aNHD40bN07Jycnq1KmTgoODqzorAAAAAFS5cpWkF198UcnJyZo5c6aSk5PVunXrqs4FAAAAALYoV0nasWNHVecAAAAAAL9QoWuSaqvAgjy7I8AH+DkDAABAoiSdUWRkpIJDQqWdK+2OAh8JDglVZGSk3TEAAABgI0rSGcTExOilF/+pvLzadYYhJydH6enpmjRpklwul91xfCoyMlIxMTF2xwAAAICNKEm/ISYmpta+aXa5XEpMTLQ7BgAAAOBTv6skrV+/Xtu2bSvzxrG33HLLHw4FAAAAAHapcEkaP368Zs6cWea8gIAAShIAAACAai2woi+YP3++3nzzTZ08eVLFxcVej7LOLAEAAABAdVLhktS4cWO1aNFCdepwORMAAACAmqfCJWnGjBkaOXJkrRvxDQAAAEDtUK7TQb179/Z6vm3bNrVu3VqtWrUqtezy5csrJxkAAAAA2KBcJenX98qpbffOAQAAAFB7lKskLViwoKpzAAAAAIBfqPA1SQ8//LBOnjxZavqxY8eUkZFRKaEAAAAAwC4VLklTp05VYWFhqel79+7VuHHjKiUUAAAAANil3ON4L1myRO+9954kacSIEQoJCfHMO3XqlFatWqV27dpVfkIAAAAA8KFyl6SioiJlZ2fLGKOcnByv+yQFBQWpb9++evDBB6skJAAAAAD4SrlL0sCBAzVw4EANGzZM8+bNk8PhqMpcAAAAAGCLCl2TdPLkSYWEhOjEiRNVlQcAAAAAbFWhkhQSEqJly5Zp7969VZUHAAAAAGxV4dHtZs6cqb/85S86cuRIVeQBAAAAAFuV+5qkEmPHjtUPP/yg+Ph4RUdHl5q/c+fOSgkGAACqv8ATR+2OAB/g54yapsIl6bbbbquCGAAAoCaqm73K7ggAUGEVLklpaWlVkQMAANRAJ5qmqLhuA7tjoIoFnjhKIUaNUuGSBAAAUF7FdRuouN7ZdscAgAopV0nq3bu33n//fYWFhal3795nXHb58uWVEgwAAAAA7FCukuRyuRQYGOj5OwAAAADUVOUqSQsWLCjz7wAAAABQ01T4Pkm/9tVXX2nJkiVyu92VkaeUKVOmKCAgoNQjPj6+SrYHAAAAoHYrV0k6cOCAbrjhBjVt2lQXX3yxVq5cKUkaP368OnTooAEDBqhly5b64IMPKj3ghAkTdOTIEa/H9ddfr169elX6tgAAAACgXCVp9OjR+uSTT9SvXz81bdpU/fr10/PPP6+MjAwtWrRI69evV+/evatkeHCHw6EGDRp4Hrm5ufrwww+Vnp5e6dsCAAAAgHJdk/Sf//xHCxcuVL9+/SRJ/fr104ABA/T444/r5ptvliQ98cQT6tKlS9Ul/f8eeOABjRkzRk2aNKnybQEAAACofcpVkg4dOqQePXp4nl9zzTUKCgrSpZde6pnWqFEjnThxovITWqxcuVJffPGFXnvttdMuU1hYqMLCQs/z/Pz8Ks0EAAAAoGYpV0kyxig1NVXBwcFe0ydOnKiwsDBJ0s8//1z56X7l/vvv17333qvw8PDTLjN9+nRNnTq1yrMAAAAAqJnKVZJSUlJ09OhRr2k9evTQTz/9pJ9++slruary1ltvKSsrSx9++OEZl5s4caLGjh3reZ6fn6+4uLgqywUAAACgZilXSfr000+rOMaZGWP00EMP6frrr1dUVNQZlw0NDVVoaKiPkgEAAACoaf7wfZJ8YdmyZcrKytJtt91mdxQAAAAANVy5ziTZ7Y033tBZZ51VpR/nAwAAQPUVWJBndwT4gK9+ztWiJH3xxRfq3r27AgIC7I4CAAAAPxIZGangkFBp50q7o8BHgkNCFRkZWaXbqBYlafPmzXZHAAAAgB+KiYnRSy/+U3l5tetMUk5OjtLT0zVp0iS5XC674/hUZGSkYmJiqnQb1aIkAQAAAKcTExNT5W+a/ZXL5VJiYqLdMWqcajFwAwAAAAD4CiUJAAAAACwoSQAAAABgQUkCAAAAAAtKEgAAAABYUJIAAAAAwIKSBAAAAAAWlCQAAAAAsOBmsn6qoKBAbrfblm3n5OR4/WkHp9Mph8Nh2/YBAABQe1GS/JTb7VZqaqqtGdLT023bdkZGRq28e3RgQZ7dEeAD/JwBAPBvlCQ/5XQ6lZGRYXcM2zidTrsj+FRkZKSCQ0KlnSvtjgIfCQ4JVWRkpN0xAABAGShJfsrhcNTKMym1VUxMjF568Z/Ky6tdZxhycnKUnp6uSZMmyeVy2R3HpyIjIxUTE2N3DAAAUAZKEuAnYmJiau2bZpfLxX8KAAAAv8HodgAAAABgwZkkAABQZRiopHbg54yahpIEAAAqHQPS1D4MSIOahJIEAAAqHQPSMCANUJ1RkgAAQJVgQBoGpAGqKwZuAAAAAAALShIAAAAAWFCSAAAAAMCCkgQAAAAAFpQkAAAAALCgJAEAAACABSUJAAAAACwoSQAAAABgQUkCAAAAAAtKEgAAAABYUJIAAAAAwIKSBAAAAAAWdewOAADwvcCCPLsjwAf4OQPA70NJAoBaJDIyUsEhodLOlXZHgY8Eh4QqMjLS7hgAUK1QkgCgFomJidFLL/5TeXm16wxDTk6O0tPTNWnSJLlcLrvj+FRkZKRiYmLsjgEA1QolCQBqmZiYmFr7ptnlcikxMdHuGAAAP8fADQAAAABgQUkCAAAAAAs+bgfUcgUFBXK73bZsOycnx+tPOzidTjkcDtu2DwAA/A8lCajl3G63UlNTbc2Qnp5u27YzMjK4RgUAAHihJAG1nNPpVEZGht0xbON0Ou2OAAAA/AwlCajlHA4HZ1IAAAAsGLgBAAAAACwoSQAAAABgQUkCAAAAAAtKEgAAAABYUJIAAAAAwIKSBAAAAAAWlCQAAAAAsKAkAQAAAIAFJQkAAAAALChJAAAAAGBBSQIAAAAAC0oSAAAAAFhQkgAAAADAgpIEAAAAABZ17A4A/1JUVKTMzEwdPnxYUVFRSkpKUlBQkN2xAAAAAJ+hJMFj1apVmjt3rvbt2+eZFhsbq5EjRyolJcXGZAAAAIDv8HE7SPqlIKWlpSkhIUFz5szR0qVLNWfOHCUkJCgtLU2rVq2yOyIAAADgE5QkqKioSHPnzlX37t01bdo0tW3bVmFhYWrbtq2mTZum7t27a968eSoqKrI7KgAAAFDlKElQZmam9u3bpyFDhigw0HuXCAwM1JAhQ7R3715lZmbalBAAAADwHUoSdPjwYUlS06ZNy5xfMr1kOQAAAKAmoyRBUVFRkqTs7Owy55dML1kOAAAAqMkoSVBSUpJiY2O1ePFiFRcXe80rLi7W4sWL1ahRIyUlJdmUEAAAAPAdShIUFBSkkSNHas2aNZo8ebKysrJ0/PhxZWVlafLkyVqzZo1GjBjB/ZIAAABQK3CfJEiSUlJSNHXqVM2dO1ejRo3yTG/UqJGmTp3KfZIAAABQa1CS4JGSkqILL7xQmZmZOnz4sKKiopSUlMQZJAAAANQqlCR4CQoKUocOHeyOAQAAANiGa5IAAAAAwKJalKRvv/1W11xzjcLDwxUREaHk5GTt2rXL7lgAAAAAaiC/L0l79+7VRRddpBYtWigrK0vr16/X0KFDFRAQYHc0AAAAADWQ31+T9Nhjj6lp06aaOXOmZ1piYqKNiQAAAADUZH5/JmnJkiXq16+f3TEAAAAA1BJ+XZJ+/PFH5ebmKjo6WsOHD1eTJk3UpUsXvf3226d9TWFhofLz870eAAAAAFBefl2Sjh49KkmaNm2akpOT9dFHH+miiy7Sn/70J61evbrM10yfPl2RkZGeR1xcnA8TAwAAAKju/LokhYSESJL+/Oc/a/jw4WrXrp2efPJJJSQk6IUXXijzNRMnTlReXp7nkZub68vIAAAAAKo5vx644ZxzzlFYWJhiY2M90wICAtSqVSsdOnSozNeEhoYqNDTUVxEBAAAA1DB+fSYpICBAF198sdasWeM1fdeuXWrdurVNqQAAAADUZH59JkmSJkyYoEsvvVQ9evTQ5Zdfrn/+85/atWuX7rnnHrujAQAAAKiB/PpMkiQlJyfrxRdf1BNPPKHmzZvrrbfe0ocffsiADAAAAACqhN+fSZKkgQMHauDAgXbHAAAAAFAL+P2ZJAAAAADwJUoSAAAAAFhQkgAAAADAolpckwQAAAD4m4KCArndblu2nZOT4/WnHZxOpxwOh23br0qUJAAAAOB3cLvdSk1NtTVDenq6bdvOyMhQYmKibduvSpQkAAAA4HdwOp3KyMiwO4ZtnE6n3RGqDCUJAAAA+B0cDkeNPZNS2zFwAwAAAABYUJIAAAAAwIKSBAAAAAAWlCQAAAAAsKAkAQAAAIAFJQkAAAAALChJAAAAAGBBSQIAAAAAC0oSAAAAAFhQkgAAAADAgpIEAAAAABaUJAAAAACwoCQBAAAAgAUlCQAAAAAsKEkAAAAAYEFJAgAAAAALShIAAAAAWFCSAAAAAMCCkgQAAAAAFpQkAAAAALCoY3cAAACAylRQUCC3223LtnNycrz+tIPT6ZTD4bBt+0BNQEkCAAA1itvtVmpqqq0Z0tPTbdt2RkaGEhMTbds+UBNQkuClqKhImZmZOnz4sKKiopSUlKSgoCC7YwEAUG5Op1MZGRl2x7CN0+m0OwJQ7VGS4LFq1SrNnTtX+/bt80yLjY3VyJEjlZKSYmMyAADKz+FwcCYFwB/CwA2Q9EtBSktLU0JCgubMmaOlS5dqzpw5SkhIUFpamlatWmV3RAAAAMAnKElQUVGR5s6dq+7du2vatGlq27atwsLC1LZtW02bNk3du3fXvHnzVFRUZHdUAAAAoMpRkqDMzEzt27dPQ4YMUWCg9y4RGBioIUOGaO/evcrMzLQpIQAAAOA7lCTo8OHDkqSmTZuWOb9keslyAAAAQE1GSYKioqIkSdnZ2WXOL5leshwAAABQk1GSoKSkJMXGxmrx4sUqLi72mldcXKzFixerUaNGSkpKsikhAAAA4DuUJCgoKEgjR47UmjVrNHnyZGVlZen48ePKysrS5MmTtWbNGo0YMYL7JQEAAKBW4D5JkCSlpKRo6tSpmjt3rkaNGuWZ3qhRI02dOpX7JAEAAKDWoCTBIyUlRRdeeKEyMzN1+PBhRUVFKSkpiTNIAAAAqFUoSfASFBSkDh062B0DAAAAsA3XJAEAAACABSUJAAAAACwoSQAAAABgQUkCAAAAAAtKEgAAAABYUJIAAAAAwIKSBAAAAAAWlCQAAAAAsKAkAQAAAIAFJQkAAAAALChJAAAAAGBBSQIAAAAAC0oSAAAAAFhQkgAAAADAgpIEAAAAABaUJAAAAACwoCQBAAAAgAUlCQAAAAAsKEkAAAAAYEFJAgAAAAALShIAAAAAWFCSAAAAAMCCkgQAAAAAFpQkAAAAALCgJAEAAACABSUJAAAAACwoSQAAAABg4fcladeuXQoICPB6NGjQwO5YAAAAAGqoOnYHKK8jR454/h4QEGBjEgAAAAA1WbUpSZw9AgAAAOALfv9xOwAAAADwpWpTkhISEnTllVdq2bJlZ1yusLBQ+fn5Xg8AAAAAKC+/L0lNmjTRt99+q5deeknt2rXTFVdcoVmzZp12+enTpysyMtLziIuL82FaAAAAANWd35ek4OBgtWjRQj169NCMGTN09913a+rUqSoqKipz+YkTJyovL8/zyM3N9XFiAAAAANWZ35ekX+vRo4eOHDmigwcPljk/NDRUERERXg8AAAAAKK9qV5J27dqlunXrqmHDhnZHAQAAAFAD+f0Q4Dt27NCePXvUokULbdiwQU8++aRSU1MVGFjt+h0AAACAasDvS1Jubq5uvvlmHTp0SC6XS2PHjtW4cePsjgUAAACghvL7ktSrVy8GXwAAAADgM3xmDQAAAAAsKEkAAAAAYEFJAgAAAAALv78mCQBQMxQUFMjtdtuy7ZycHK8/7eB0OuVwOGzbPgCg/ChJAACfcLvdSk1NtTVDenq6bdvOyMhQYmKibdsHAJQfJQkA4BNOp1MZGRl2x7CN0+m0OwIAoJwoSQAAn3A4HJxJAQBUCwzcAAAAAAAWlCQAAAAAsKAkAQAAAIAFJQkAAAAALChJAAAAAGBBSQIAAAAAC0oSAAAAAFhQkgAAAADAgpIEAAAAABaUJAAAAACwoCQBAAAAgAUlCQAAAAAsKEkAAAAAYEFJAgAAAAALShIAAAAAWFCSAAAAAMCCkgQAAAAAFnXsDlDVjDGSpPz8fJuTAAAAALBTSSco6QinU+NL0rFjxyRJcXFxNicBAAAA4A+OHTumyMjI084PML9Vo6q54uJi7dmzR+Hh4QoICLA7TrWQn5+vuLg45ebmKiIiwu44qMHY1+Ar7GvwFfY1+Ar72u9jjNGxY8fUuHFjBQae/sqjGn8mKTAwUOeee67dMaqliIgI/tHBJ9jX4Cvsa/AV9jX4CvtaxZ3pDFIJBm4AAAAAAAtKEgAAAABYUJJQSmhoqNLS0hQaGmp3FNRw7GvwFfY1+Ar7GnyFfa1q1fiBGwAAAACgIjiTBAAAAAAWlCQAAAAAsKAkAQAAAIAFJcnHEhIS9NBDD3lN69atm7p06eI17emnn9YVV1zxm+sLCAjQwoULKzPiaR0/flxTp05VmzZtVLduXYWHh6tVq1a68cYbq3S7Cxcu5EbAv8P27dvVt29fhYeH66yzzlJycrLcbnelrDs+Pl4BAQFej759+1bKun+PXbt2KSAgQJ9++qltGfDbevXq5dlf6tatqy5dumjlypWe+fHx8ZoyZUq51nXbbbepV69eVRO0kn366acKCAjQ5s2b7Y4CSQcPHtSoUaN07rnnKjQ0VM2aNdPkyZN1/Pjxcq/jt469vXr10m233fbHw57Bbbfdpk6dOlXpNvDHVeWxeMqUKTr77LMrZV0orcbfTNbfJCcna/Xq1Z7nhYWF+vLLLyVJJ0+eVEhIiCRp+fLl6t27ty0Zly1bpt27d2vYsGGeaQUFBbr44ou1b98+PfXUU0pOTtbRo0e1ceNGvfPOO7bkxOn9/PPPuuKKK9S+fXt9/fXXOnbsmN5//30VFxdX2jbuvvtuPfLII57nwcHBlbbuinI6nTpy5Ijq169vWwaUz7XXXqsFCxYoOztb48ePV//+/eV2uxUeHq7MzEzP70CgKhw6dEjdu3dXw4YN9frrr6tZs2bKzMzUPffcoxUrVmjFihXsg6g0vjgWo+pQknwsOTlZY8aM0alTp1SnTh19+eWXCgkJ0Y8//qhNmzapS5cuKioq0qpVq5SWlmZLxpdfflm7du3yKkkzZszQ+vXrtXnzZrVu3VqSFBsbq1atWmnw4MFlrscYo1OnTtn65rm2ysrK0o4dO/TSSy8pPj5ektSuXbtK3UZoaKgaNGjwm8sVFhZW+fCkgYGB5coC+wUHBysqKkpRUVGaNm2akpOTtW3bNnXq1Ik7xqPKTZw4UUeOHNHGjRs9+9ull16qDz/8UImJiZo5c6YmTJhgc0rUFL44FqPq8HE7H0tOTtbx48e1ceNGSdLatWt17bXX6qyzztK6deskSevXr1dgYKA6duyoF154QR07dlS9evUUGxurJ5988ozrj4+PV1pamtLT0xUTE6OYmBi9+uqrnvllfXTN+rGAKVOmaNGiRVq5cqUCAgI80xctWqRrrrnGU5DOtP1JkyZpxIgRCg8P19q1ayVJGzZsUNeuXRUWFqYuXbpo06ZNXq9btmyZzjvvPNWvX1+XXHKJdu3addptvP/++6pfv36pdeD/REVFKSAgQPPnzy/1P1bHjx/X7bffLpfLpbp166pDhw6e/VH6v31ky5YtSklJUb169dS3b1/l5+eXa9slr1+9erXatGmja665RpL07rvvqkePHoqIiFB0dLTGjBnj9brf2ncl6aOPPlKXLl3kcDgUERGh22+/XVLZH7fLyMiQ0+lUSEiIEhISdOLECc+8ffv2aciQIYqOjlZoaKiaN2+uffv2SZLy8/N1++23Kzo6WmFhYbr88su1bdu2cn3tqJhDhw5Jkho2bCip9Mftjh07ppEjRyo2NlYhISFyOp2eM+8l3nnnHbVq1UoREREaN26c17yAgABlZGTonnvuUWRkpJo2barPPvtMn376qdq2bavw8HDdc889Xq/ZunWrLr/8coWFhSk6Olp33HGHjh075pk/ZcoUxcfH67///a86duyo+vXra8iQISoqKvIsk5OTo0suuUQOh0Pt2rXTf//730r5fuGPOXnypBYvXqzhw4eXKuRNmzbVddddpxdeeEGSKnTsPXr0qBITE/Xggw+edpnK+P336quvKj4+XmFhYRo8eLB+/PHHin4L4GO+PhYbYzRjxgy5XC6Fhoaqffv2Wrp0qdcyZzo2Hjx4UAMHDlS9evXUvHlzvfzyy5X0naimDHyquLjYREdHmyeffNIYY8yNN95onnnmGXPppZeaW265xRhjzKOPPmquvfZaY4wxCxcuNKtXrzZ79+41TzzxhAkICDC5ubme9UkyCxYs8Dx3uVymUaNGZtiwYSY7O9uMGzfOhIeHm59//tkYY8yCBQvMr3/sPXv2NLfeeqsxxpgTJ06Ym266yVx44YXmyJEj5qeffjJ5eXlGkpkyZcpvfn0ul8ucffbZJi0tzeTm5prjx4+bo0ePmrPPPts8/fTTJjc314wfP97Ex8d7Mu3cudPUrVvXvPrqq8btdpvBgweb5ORkzzqtmbdu3WoiIyPN4sWLK/Jtr5XGjBljJJkePXqYzZs3e6YfPXrU/O1vfzNbtmwxOTk5Jjk52fTp08czv+T73b59e7N06VKzZcsW06BBAzNjxgzPMi6Xy4wbN67M7Za8Pjk52axZs8YcOHDAGGPMm2++af7973+bPXv2mFdeecVIMqtXr/Za55n23a+++soEBQWZ66+/3mzdutXs3LnTfPbZZ8YYY7Kzs40ks2LFCmOMMW632wQGBpqlS5eavXv3mn//+9+e9RQXF5suXbqYJk2amGXLlpk9e/aYFStWmGPHjhljjBk4cKBp166dWbdundm+fbu58cYbTXx8vDlx4sQf/ZHUej179jQDBgwwxcXFZsOGDaZNmzZm4MCBnvkul8ukpaV5ng8cONBERkaaN954w+zZs8esWbPG7N271xhjzK233mrOOeccc+GFF5pNmzaZN99800gya9eu9bxekomLizPp6enG7XabLl26mPj4eJOcnGw2bNhgFi5caCSZjRs3GmOMOX78uImLizM33nij2b59u1m3bp1p166dGTRokGedaWlpJjw83HTs2NGsXr3afPHFFyYwMNC89tprnmU6d+5sLr/8cvPtt9+a5cuXm/bt2xtJ5uuvv66qby3KITMz00gyL774YpnzH3nkESPJHDt2rNzH3qKiInPllVeavn37mqKiIs9863HVmD/++y8zM9MEBQWZZ555xmRnZ5vZs2ebsLAwc8EFF1TydwmVrSqPxWlpaSY6Otrz/NlnnzWRkZHm7bffNjk5Oebxxx83QUFBnt89Zzo2GmPMpZdeagYPHmzcbrd5/fXXTVBQkNm2bVtVfnv8GiXJBtdcc43p37+/McaY+Ph4s27dOvPggw+aVq1aGWOM6dOnj/nb3/5W6nU7d+40ksyqVas808oqSZdeeqnn+bJly4wk43a7jTG/XZKM+eXNR8+ePT3Pc3NzjSTz3HPP/ebX5nK5vN5QGGPMU089ZVq0aOF5fvjwYa8DxD333OOVeePGjUaS+f77770y5+XlmVatWpkxY8b8Zg78Yu7cuaZ+/fomJCSkzH3KGGMeeughk5CQ4Hle8v3+/PPPPdN69Ohhhg8f7nnucrlMaGioiYyM9Dx+/PFHr9dv2bLltLmKi4uNJPPPf/7Ta51n2neHDBliGjZsaI4fP15qfb8uSZs2bTKSzPvvv19q2U8++cRIMm+++Waped99952RZD766CPPtCNHjpjQ0FDzwgsvnPbrQfn07NnTBAcHm5CQECPJ3HrrrV7l01qSvv32WyPJPPXUU2Wu69ZbbzUxMTHmhx9+MMYYc/LkyVL7lCQzfvx4z/MxY8aY8PBwT9Hav3+/177w/PPPG4fDYY4ePep5zYcffmgkme+++84Y88ubEofD4XlujDGNGzc2Dz/8sDHGmM8++6zU/r9ixQpKkh8o+dlY/31bPffcc17HnhJnOvZOnDjRJCQkmMOHD3u95tfHVavf8/vv9ttvN127dvVaz6233kpJqiaq6lj865J07rnnmgkTJnitt2vXrubmm282xpz52LhhwwYjyezevdszLSkpyUybNq2CX23NwcftbJCcnKzPP/9c+/fv1/79+9W+fXt17txZ27Zt08GDB/X555/r4osvliR99913uuuuu9SuXTvPKDbWj3WUpUePHp6/l1wP9FuvOZOSjyWUfBzpt7Rq1crr+ddff63s7Gw1aNBADRo0UNOmTSVJubm5nvkrV670zO/Zs6fX/BK33HKL6tatq8cff/x3fy21zYgRI7R161b16tVLo0eP9pw6X7Jkifr27atmzZrp8ccfL3P/+PV+9OtlbrnlFm3atMnzCAsL85r/6/1g7969uu+++9ShQwfFxsZKKr1fnmnfzcrKUrdu3VS3bt3f/Lrbt2+vQYMG6ZprrlH//v29RhXLysqSJM+/MauSee3bt/dMa9CggVwul2ce/pg+ffpo27Zt6tatm7Zv337a69W++eYbSWX/nEq0atVKUVFRkk7/u65t27aevwcGBioqKsqz/wUG/nIILPkYTFZWllwulyIjIz2vKdkXSvJIUkxMjJo1a+Z5bv338fXXXys4OLjU/g/7lVy3ePDgwTLn79+/X5IUGRlZrmPvkiVL9Nhjj+mNN97QWWeddcZt/9Hff19//TXXslRjVXksLpGfn6/vv//e6/gl/fI7zHpsO92x8euvv5YktWnTxvN+LCsrq9R7sdqEkmSD5ORkHTx4UIsWLVLHjh0VHByszp07yxijZ599VvXq1VO7du108OBBde3aVdu3b9esWbO0ZMkSW/JGRETI5XJ5rpmqqJMnT+q8887zekOdnZ2tq6++2jP/sssu88zLzMxUdna2zj//fK/1XHDBBcrKyuLz/RXUpEkTffDBBzrvvPO0cOFC/eMf/9DAgQPVrVs3vfnmm7rzzjt/13ojIiIUHx/veZxpmPaTJ0/qoosu0n/+8x9NmzZNn3/+eYW3V6dOnQoNBf/qq69qyZIlys3NVefOnT3brFPnl/FqKrIuYwzD0FeSsLAwxcfHa/bs2VqzZo3mz59f5nK/5+dUFYwxFcpx7NixKh+oBL9Py5YtVa9ePX311Vdlzs/MzFTz5s114sSJch17ExMTFR4eroyMjDNutzJ+/x07dkwOh6PCr4P/qKpj8W/59fHrdMfGkydPKjAwUGvWrPG8H/vuu+80bdq0KslVHVCSbNCpUyc5HA7Nnj1b3bp1k/TLP57GjRtrzpw5nnuJrFixQocPH9brr7+u3r17y+l0/uFtBwUFSfplxDFJys7OLjUAQlBQkE6dOuU1bciQIVq6dOnvKkpdu3bVli1bFBoa6vWmumS45q5du2r9+vVq2LCh1/xfHxD++te/6vrrr9ef/vQn7dixo8I5arM6deqoRYsWCgsL07/+9S9deeWVmjJlis4//3yfjAq3adMm7dixQ/Pnz9dVV12l5s2bV3gdnTp10hdffOF1kelv6devn9auXatzzz3XcxF0yf8Kr1ixotTyJWcdMjMzPdOOHDkit9vtdUYCf1znzp01bNgwTZgwQQcOHCg1//zzz1dQUFCZP6eqct555yknJ0d5eXmeaSVvqNu0aVOudbhcLv34449eX5N1fbBPcHCwhg4dqn/84x9eg3FIvxwL33nnHQ0fPrzcx962bdvqtdde0/PPP6+//e1vp91uZfz+c7lcpY577FfVT1UeiyMiIhQXF+d1/JJ++R326+NXWcfGrl27qri4WFu2bPF6L1ab78NESbJBSEiIOnfurN27d3tKkvTLm4YffvjBc3+kuLg4SdIHH3ygLVu26L777lOdOnW0cePGUiWmvFq2bClJmjlzpl5++WVdffXVpf5hNm/eXJs2bdKGDRt09OhRSdKDDz6odu3a6eqrr9aiRYu0e/dubd++Xa+88oouuuiiM25z2LBhaty4sQYMGKD//e9/2rt3rz755BPPP+QxY8aosLBQN954ozIzM7V79269++67ZY5w98ILL6hly5a6+uqrPdlQ2vLlyzV69Gh9/vnn2rNnj9544w0tXbpUt956q+Li4pSVlaVt27Zp6dKleumll3TgwIEzjij4RzVu3FiBgYH6+OOPtWPHDo0aNUohISHatGmTTp48Wa51TJw4UceOHdPNN9+sbdu2aceOHaf9H95169Zp/vz5ys7O1saNG3Xo0CHPyIxdu3bVVVddpXvuuUfLli3Tnj17tHTpUu3bt0/NmjXTDTfcoPvuu09ffvmltm/frjvvvFONGjXSTTfdVGnfD/zi0UcfVVFRkcaOHVtqXpMmTXTXXXdp6tSpev3117Vnzx59+umn+vbbb6ssz+DBgxUTE6ORI0dqx44dWrdune677z7deOONSkhIKNc6rrzySjVo0EATJkxQbm6ulixZcsZRz+Bbjz76qKKjo3XFFVfoiy++0IEDB7Rs2TJdccUV6tSpk8aOHVuhY2/fvn319NNPa/z48aVGEStRGb//hgwZon//+99666235Ha7dd9993ndcxH+yRfH4pKz7tIvx8l58+bp/fffV25urh577DFt2LBB999/v6QzHxuTkpI0YMAA3X333Xr33Xe1d+9erVu3Th9//HGlfT+qHXsviaq9Jk6caCR5jZYzbdq0Uhf8PvDAAyYiIsK0bNnSvPPOO2bMmDEmLCzMrFmzxhhT9sAN1tGhSi4Yzs7O9kwbPXq0CQ8PN23atDHLly8vdYHpkSNHzGWXXWbCwsLMgw8+6Jmel5dnJkyYYJo3b25CQkJMdHS06d69u5k5c+Zpt1/C7XabgQMHmoiICFOvXj1zwQUXeEYmM8aYr7/+2lx++eUmLCzMREREmB49epitW7caY0oPNnHgwAHTtGlT06dPH69RWfB/Nm3aZHr27GmioqJMWFiY6dChg3nllVeMMcbs3r3bJCcnm7p165p+/fqZzZs3m+bNm5v27dsbY8o3uEd5Rrf7tdmzZ5vo6GgTFxdn5s+fb55++mnjcDjM66+/7lnnb+27n332mbngggtMSEiIadSokbn77ruNMWUP3NC9e3dTr14907BhQzN27Fhz6tQpz3qOHj1qbrnlFhMeHm7Cw8NNt27dTE5OjjHml/182LBhJjIy0tStW9dcfvnlnn0Rf0zJ6HZWTz75pJFkPvnkk1L7QGFhofnLX/7itR+vX7/eGFN6gBljSv8+/PXzcePGGZfL5Xl+8OBBI8m88cYbnmlbtmwxffr0MQ6Hw0RFRZk77rjD5Ofne+anpaV5rcOY0vvuZ599ZpKSkjz7z8svv8zADX7kwIEDZsSIEaZRo0YmODjYJCQkmAcffNAz+IwxFT/2jho1yoSHh5vMzExjTOnfmX/0919xcbF56KGHTGxsrImOjjZTp05l4IZqoKqPxaNHjzYdO3b0Wmb69OmmSZMmJjg42LRv39588MEHXnnOdGwsKCgw48aNM7GxsSYkJMS0aNHCzJ07t5K/K9VHgDH//wPXAAAAAPzWkSNHVK9ePZ04cULdunXTddddp0cffdTuWDUSH7cDAAAAqoE333xTYWFhatiwodq2basJEybYHanG4kwSAAAAUA0UFxdr//79ioqKYiTNKkZJAgAAAAALPm4HAAAAABaUJAAAAACwoCQBAAAAgAUlCQAAAAAsKEkAAFv16tVLAwcOLDV9/Pjxio+P90mGXbt2KSAgQJ9++qmt6wAA+AdKEgCg1nM6nTpy5IiSk5M905544glt3LjRxlQAALtQkgAAtV5gYKAaNGigOnXqeKbdf//9yszMtDEVAMAulCQAgN8zxmjGjBlyuVwKDQ1V+/bttXTpUq9lMjIy5HQ6FRISooSEBJ04ccIzLz4+XpMnT9a9996rBg0aqGHDhpo5c6Zn/q8/KlfyMb9hw4Z5TZ8+fbratm2rsLAwuVwuvfTSS1X6dQMA7EFJAgD4vblz52r69OmaPXu2tm/frqFDh+raa6/V5s2bJUm5ubkaMWKE/v73v8vtdmv+/PkKDg72Wsfzzz+vJk2aaPPmzXr44Yc1btw4/fe//y1zeyVnkObMmeP1MbyIiAgtWrRIO3fu1HXXXafU1FT9/PPPVfiVAwDsQEkCANju3XffVYMGDbwezz77rGf+Y489phEjRqhfv35yOp2677771KlTJz3++OOSpMOHD6u4uFjFxcWKjY3VJZdc4vXROUm66aab9MADD+jcc8/VXXfdpdatW+vFF18sM09ERIQkKSwszOtjeKNGjVKnTp0UGxurfv366cSJE9q3b19VfEsAADaiJAEAbNenTx9t2rTJ63HLLbdIkvLz8/X999+rffv2Xq9p3769srKyPH8fNGiQrrnmGvXv399zhskqMjLS63lCQoLcbneFcn755ZcaOnSo2rRpo/79+0uSioqKKrQOAID/oyQBAGwXFham+Ph4r0fJ2ZzTMcYoICDA8/zVV1/VkiVLlJubq86dO+vzzz8/4+sLCgoUHh5e7oxbtmxR9+7d9fPPP2v+/Pl67rnnyv1aAED1QkkCAPi1iIgIxcXFlRpp7quvvlLbtm29pvXr109r167Vueeeq1dfffW06ywuLtY333yj884777TLBAYG6tSpU57n7733nkJDQ/XKK6/owgsvVKNGjX7nVwQA8Hd1fnsRAADsNXHiRD344IPq0aOH2rdvr8WLF2vDhg36xz/+IUlat26dNm3apD59+ujgwYM6dOiQWrdu7bWOZ555Rs2aNVNKSor+/ve/Ky8vT6mpqafdZvPmzbVkyRJdccUVOvvssxUXF6effvpJy5cvV3R0tNLT0yVJ69evl9PprLovHgDgc5QkAIDfGzFihPLy8nTXXXfpwIEDatOmjd59913PmaSQkBAtWLBAY8aMUb169TR8+HDdeeedXusYOHCgXnnlFf35z3+Wy+XS22+/rYYNG552m88++6xSU1PVpk0brVq1SjfccIM+/vhj9e/fXy1bttQzzzyjyMhIDR8+XF27dq3Srx8A4FsBxhhjdwgAAKpSfHy8brvtNk2ZMsXuKACAaoBrkgAAAADAgpIEAAAAABZ83A4AAAAALDiTBAAAAAAWlCQAAAAAsKAkAQAAAIAFJQkAAAAALChJAAAAAGBBSQIAAAAAC0oSAAAAAFhQkgAAAADAgpIEAAAAABb/D9xCZWPTqgldAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='hospital', y='weight', data=df)\n",
    "plt.xlabel('Hospital')\n",
    "plt.ylabel('Birth Weight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oakland has the lowest overall birthrate because its mean is the lowest."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3  ( / 12pt)\n",
    "Create a set of 4 dummy variables that together code the hospital. Set Walnut Creek to be your comparison group. Run a multiple regression model with the 4 dummy variables as explanatory variables. Report the interecept and slope values. What do the intercept and slope values mean? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept (b0): 7.645779794204446\n",
      "San Francisco coefficient (b1): 0.021341880492077674\n",
      "Richmond coefficient (b2): -0.33250686243896915\n",
      "Oakland coefficient (b3): -0.20435329740744085\n",
      "San Jose coefficient (b4): -0.1012797795285041\n",
      "R-squared: 0.017073994697972217\n"
     ]
    }
   ],
   "source": [
    "# Create dummy variables for hospitals, excluding Walnut Creek (reference group)\n",
    "df['hosp_sf'] = (df['hospital'] == 'SanFrancisco').astype(int)\n",
    "df['hosp_richmond'] = (df['hospital'] == 'Richmond').astype(int)\n",
    "df['hosp_oakland'] = (df['hospital'] == 'Oakland').astype(int)\n",
    "df['hosp_sanjose'] = (df['hospital'] == 'SanJose').astype(int)\n",
    "\n",
    "# Run regression with hospital dummies\n",
    "xname = ['hosp_sf', 'hosp_richmond', 'hosp_oakland', 'hosp_sanjose']\n",
    "R2, b = multRegFit(df, df['weight'], xname)\n",
    "\n",
    "print(f\"Intercept (b0): {b[0]}\")\n",
    "print(f\"San Francisco coefficient (b1): {b[1]}\")\n",
    "print(f\"Richmond coefficient (b2): {b[2]}\")\n",
    "print(f\"Oakland coefficient (b3): {b[3]}\")\n",
    "print(f\"SanJose coefficient (b4): {b[4]}\")\n",
    "print(f\"R-squared: {R2}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intercept represents the average birth weight at walnut creek. Each slope represents the difference in birth weight between a hospital and walnut creeek."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Model selection for multiple regression  ( / 35 pts)\n",
    "### Question 2.1 ( / 20 pts)\n",
    "Write a version of the crossvalidation function that does K-fold crossvalidation and works specifically with multRegFit as the fitting function. \n",
    "\n",
    "KfoldCVmultReg(D,y,xname,K=20,fitfcn=multRegFit,param={},predictfcn=multRegPredict):\n",
    "- D: Data Frame with explanatory variables  \n",
    "- y: response variable \n",
    "- xname: List of explanatory variables\n",
    "- K: Number of crossvalidation folds\n",
    "\n",
    "For dividing the data up in K pieces, you can use the following trick to assign a partition index to each of the data-points:\n",
    "```\n",
    "#N = number of data points \n",
    "#K = number of test sets (folds)\n",
    "ind = np.arange(N)\n",
    "ind = np.floor(ind/N*K)\n",
    "```\n",
    "\n",
    "\n",
    "The code should compute and return the predictive (crossvalidated) $R^2$ (R2cv) and the fitted $R^2$ (R2). \n",
    "It should use the entries in the Dictionary to pass them to the function using `fitfcn(D,y,xname,**param)`\n",
    "Run 20-fold crossvalidation on the multiple regression model, with birth weight as the response variable and \n",
    "\n",
    "- age of the mother \n",
    "- smoker (dummy coded) \n",
    "- birth occurred in Oakland? \n",
    "- gestation \n",
    "\n",
    "as explanatory variables. \n",
    "Report R2cv and R2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moakland_dummy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhospital\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOakland\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     35\u001b[0m xname \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmokeDummy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moakland_dummy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgestation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 37\u001b[0m R2cv, R2 \u001b[38;5;241m=\u001b[39m \u001b[43mKfoldCVmultReg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCross-validated R-squared (R2cv): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mR2cv\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitted R-squared (R2): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mR2\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 20\u001b[0m, in \u001b[0;36mKfoldCVmultReg\u001b[0;34m(D, y, xname, K, fitfcn, param, predictfcn)\u001b[0m\n\u001b[1;32m     17\u001b[0m     y_train \u001b[38;5;241m=\u001b[39m y_array[train_indices]\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Fit model and make predictions\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     R2_train, b \u001b[38;5;241m=\u001b[39m \u001b[43mfitfcn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     ypred[test_indices] \u001b[38;5;241m=\u001b[39m predictfcn(D_test, b, xname)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Calculate R2cv using numpy arrays\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 56\u001b[0m, in \u001b[0;36mmultRegFit\u001b[0;34m(D, y, xname, figure, b0)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(b0)\u001b[38;5;241m!=\u001b[39mk):\n\u001b[1;32m     55\u001b[0m     b0\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros((k,))\n\u001b[0;32m---> 56\u001b[0m RES \u001b[38;5;241m=\u001b[39m \u001b[43mso\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmultRegLossRSS\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb0\u001b[49m\u001b[43m,\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mxname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m b\u001b[38;5;241m=\u001b[39mRES\u001b[38;5;241m.\u001b[39mx \u001b[38;5;66;03m# Results\u001b[39;00m\n\u001b[1;32m     58\u001b[0m res \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean(y)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/scipy/optimize/_minimize.py:708\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    706\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_cg(fun, x0, args, jac, callback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbfgs\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 708\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_bfgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton-cg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    710\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    711\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/scipy/optimize/_optimize.py:1372\u001b[0m, in \u001b[0;36m_minimize_bfgs\u001b[0;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, finite_diff_rel_step, xrtol, c1, c2, hess_inv0, **unknown_options)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxiter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1370\u001b[0m     maxiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(x0) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[0;32m-> 1372\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m f \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun\n\u001b[1;32m   1376\u001b[0m myfprime \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/scipy/optimize/_optimize.py:288\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    284\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/scipy/optimize/_differentiable_functions.py:166\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl \u001b[38;5;241m=\u001b[39m update_fun\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Gradient evaluation\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(grad):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/scipy/optimize/_differentiable_functions.py:262\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/scipy/optimize/_differentiable_functions.py:163\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/scipy/optimize/_differentiable_functions.py:145\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/scipy/optimize/_optimize.py:79\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m     78\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/scipy/optimize/_optimize.py:73\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 73\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m, in \u001b[0;36mmultRegLossRSS\u001b[0;34m(b, D, y, xname)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmultRegLossRSS\u001b[39m(b,D,y,xname):\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loss function for OLS multiple regression\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m        grad: gradient of loss function in respect to parameters\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     predY \u001b[38;5;241m=\u001b[39m \u001b[43mmultRegPredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43mxname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     res \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m-\u001b[39mpredY\n\u001b[1;32m     32\u001b[0m     rss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(res\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m, in \u001b[0;36mmultRegPredict\u001b[0;34m(D, b, xname)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmultRegPredict\u001b[39m(D, b, xname):\n\u001b[0;32m----> 2\u001b[0m     yp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mlen\u001b[39m(D)) \u001b[38;5;241m*\u001b[39m \u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(xname)):\n\u001b[1;32m      4\u001b[0m         yp \u001b[38;5;241m=\u001b[39m yp \u001b[38;5;241m+\u001b[39m D[xname[i]] \u001b[38;5;241m*\u001b[39m b[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "def KfoldCVmultReg(D, y, xname, K=20, fitfcn=multRegFit, param={}, predictfcn=multRegPredict):\n",
    "    N = len(y)\n",
    "    ind = np.arange(N)\n",
    "    ind = np.floor(ind/N*K)\n",
    "    \n",
    "    ypred = np.zeros(N)\n",
    "    y_array = y.to_numpy()  # Convert to numpy array\n",
    "    \n",
    "    for k in range(int(K)):\n",
    "        # Create boolean masks and convert to indices\n",
    "        test_indices = np.where(ind == k)[0]\n",
    "        train_indices = np.where(ind != k)[0]\n",
    "        \n",
    "        # Use integer indexing\n",
    "        D_train = D.iloc[train_indices]\n",
    "        D_test = D.iloc[test_indices]\n",
    "        y_train = y_array[train_indices]\n",
    "        \n",
    "        # Fit model and make predictions\n",
    "        R2_train, b = fitfcn(D_train, y_train, xname, **param)\n",
    "        ypred[test_indices] = predictfcn(D_test, b, xname)\n",
    "    \n",
    "    # Calculate R2cv using numpy arrays\n",
    "    R2cv = 1 - np.sum((y_array - ypred)**2) / np.sum((y_array - np.mean(y_array))**2)\n",
    "    \n",
    "    # Calculate R2 on full dataset\n",
    "    R2, _ = fitfcn(D, y, xname, **param)\n",
    "    \n",
    "    return R2cv, R2\n",
    "\n",
    "# Prepare the data\n",
    "df['smokeDummy'] = (df['smoke'] == 'Smoker').astype(int)\n",
    "df['oakland_dummy'] = (df['hospital'] == 'Oakland').astype(int)\n",
    "\n",
    "xname = ['age', 'smokeDummy', 'oakland_dummy', 'gestation']\n",
    "\n",
    "R2cv, R2 = KfoldCVmultReg(df, df['weight'], xname)\n",
    "\n",
    "print(f\"Cross-validated R-squared (R2cv): {R2cv:.3f}\")\n",
    "print(f\"Fitted R-squared (R2): {R2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2 ( / 15 pts)\n",
    "Using the R2cv from the 20-fold crossvalidation, determine the best predictive model for birthweight using the following candidate variables \n",
    "\n",
    "- age of mom\n",
    "- smoker (dummy coded) \n",
    "- gestation \n",
    "- parity \n",
    "\n",
    "Start with the R2cv for the full model (Question 2.1)and use **backwards** step-wise regression to find the best model (the model that increases R2cv the most). Show all steps of your selection procedure. Report the formula of your best model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Implement regularized regression to build a better predictive model (/35pts)\n",
    "In this task you will implement regularized regression to try to build a better predictive model for the birthweight of data. \n",
    "Like in Task 2, we will consider the following explanatory variables:\n",
    "\n",
    "- age of the mother \n",
    "- smoker (dummy coded) \n",
    "- birth occurred in Oakland? \n",
    "- gestation "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1: Z-standardize the regressors (/8pts)\n",
    "Write a function `zstandardize`, which takes as an input a pandas series or ndarray \n",
    "and returns a z-standardized version of the data \n",
    "\n",
    "Use the function to z-standardize the columns age,gestation,parity, and smokeDummy. \n",
    "\n",
    "Create new columns in the data frame called ageZ,gestationZ,parityZ, and smokeDummyZ.\n",
    "\n",
    "Check that the mean of the new variables in very close to and the std very close to 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2 Implement Ridge regression (L2 regularized regression) (/17pts)\n",
    "\n",
    "To implement ridge regression you need to modify two functions, the most important being the loss function. \n",
    "Make a copy of the function `multRegLossRSS` from assigment 10. \n",
    "Rename it to `ridgeLoss`. Give the function an additional input parameter, namely alpha. Give this a default value of 1.0. \n",
    "\n",
    "Change the loss and the gradient to take into account the regularization. \n",
    "\n",
    "**Note that we are not regularizing the intercept regressor (b0)**\n",
    "\n",
    "Overall the function should take the following input arguments:\n",
    "\n",
    "    Args:\n",
    "        b (nd.array): Array of regression coefficients - first is intercept \n",
    "        D (pd.DataFrame): Pandas data frame with explanatory variables\n",
    "        y (ndarray): Dependent variable \n",
    "        xname (list): List of strings with names of explanatory variables\n",
    "        alpha (float): Regularization parameter \n",
    "\n",
    "    Returns:\n",
    "        loss: Current loss\n",
    "        grad: gradient of loss function in respect to parameters  \n",
    "\n",
    "Then make a copy of `multRegFit` from the last homework and rename it to `ridgeFit`. \n",
    "Again, you need to add an additional input parameter (alpha) to the function. \n",
    "Alpha needs to be passed to your loss function (`ridgeLoss`) when you call so.minimize: \n",
    "\n",
    "`so.minimize(ridgeLoss,b0,args=(D,y,xname,alpha),jac=True)`\n",
    "\n",
    "You need to take care when you calculate R2 of the fit - Since ridgeLoss does not return the \n",
    "residual-sum-of-squares, you need to use the appropriate function to calculate the RSS.\n",
    "\n",
    "To test your function:\n",
    "* Use it to fit a model that explains weight with the explanatory variables `['ageZ','smokeDummyZ','gestationZ','parityZ']`. Note: use the zstandardized version of the variables.  \n",
    "* Do the fit setting `alpha=0` and `alpha=8`, and report both R2 and the regression coefficients (b)\n",
    "* Compare the R2 between the two settings of alpha. Also compare to the one found for normal multiple regression (Question 2.1). What do you see and why?\n",
    "* Compare the regression weights (b) between the two settings of alpha. which regression weights changed and why? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.3: Crossvalidate Ridge regression (10pts)\n",
    "Copy your function `KfoldCVmultReg` from Question 2.1, rename it to `KfoldCVridge`, and modify it to work with Ridge regression. \n",
    "That means it needs to take an additional input parameter `alpha` that it passes on to the fitting function. \n",
    "\n",
    "To calculate the R2 and R2cv for the model of `weight` using the explanatory variables `['ageZ','smokeDummyZ','gestationZ','parityZ']`. Like in question 3.2, use the standardized versions of the variables and try both the setting `alpha=0` and `alpha=8`. \n",
    "\n",
    "How to the R2 and R2cv values compare between the two settings of alpha? Do you get better predictive performance than the reduced model that you found using feature selection (Question 2.2)?\n",
    "\n",
    "\n",
    "*Note: If you want, play a bit with the regularization parameter to see if you can find a better setting. What happens when you make `alpha` very large (i.e. 1000)? (this is optional, but educational)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Use logistic regression to predict complications \n",
    "In this task you will create and test a logistic regression model that predicts the presence of a complication in the first three month (0: no complication, 1: complication). \n",
    "\n",
    "**Task 4 of the Homework does not have to be handed in and will not be graded! It is only added here to provide additional preparation and practice for you for the final. So if you are short on time, leave these questions open and solve them when you practice for the final.**\n",
    "\n",
    "### Question 4.1: Improving your logistic regression model code\n",
    "Improve your code for logisitic regression in two ways: \n",
    "\n",
    "1. prevent log(0) errors by making sure that your predicted value never is smaller than 1e-20 or larger than 1-1e-20. (tip you can use the numpy function `clip`)\n",
    "\n",
    "2. Let logisticRegFit take an additional input parameter, telling it whether it should plot a figure or not (figure=1) \n",
    "\n",
    "3. Let logisticRegFit take an additional input parameter, specifying the starting value for the parameters (b_init=[]). If b_init is empty, the function should start with a vector off all zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2: Crossvalidation of logistic models\n",
    "Modify the KfoldCVmultReg function to make it work for logistic regression. As before, use K-fold crossvalidation. The main changes are that \n",
    "- you need to use logisticRegFit and logisticRegPredict as fitfcn and predictfcn respectively. \n",
    "- To save time, initialize each optimization with from the parameters that you found on the entire data by setting `b_init` \n",
    "- instead of the crossvalidated R2, your function should return the crossvalidated log-likelihood and non-crossvalidated log-likelihood. \n",
    "\n",
    "Using your function, calculate the the difference in crossvalidated log-likelihood for the model that predicts complications with an intercept only (b0) and a model that predicts complications with an intercept and smokeDummy. \n",
    "From the difference, report the Bayes-Factor between the two models. What do you conclude? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.3:\n",
    "Compare the model that uses only intercept, smoking as explanatory variable to one that uses: \n",
    "\n",
    " * intercept, smoking, weight \n",
    " * intercept, smoking, age \n",
    " * intercept, smoking, weight, age \n",
    " \n",
    "Report the cross-validated Log-likelihood for each model. Which one is the best model? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.4 : \n",
    "In the model (['smokeDummy','weight']), how do each of the explanatory variables contribute to the chance of complication? That is, for each variable, would an increase an the variable lead to an increased or decreased probability of a complication? \n",
    "\n",
    "What reduction in birth weight causes the same amount of risk of complication as having a smoking mother?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
